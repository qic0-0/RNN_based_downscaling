{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb20fe9d-0dbb-4caf-aa2b-8b5921383169",
   "metadata": {},
   "source": [
    "## AEP"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-31T00:33:52.692817Z",
     "start_time": "2025-10-31T00:33:50.538645Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from prophet import Prophet\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "id": "8118ffd26142a9a2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Importing plotly failed. Interactive plots will not work.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-31T00:33:52.765693Z",
     "start_time": "2025-10-31T00:33:52.702458Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_aep = pd.read_csv(\"data/AEP_hourly.csv\")\n",
    "df_aep.rename(columns={'Datetime': 'ds', 'AEP_MW': 'y'}, inplace=True)\n",
    "df_aep['ds'] = pd.to_datetime(df_aep['ds'], format='%Y-%m-%d %H:%M:%S')\n",
    "out_dir = 'AEP_results'"
   ],
   "id": "5db8d797fb6b5d60",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### test prophet based without seasonality",
   "id": "e20100736d301702"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import prophet_linear_adjust as prophet_based\n",
    "daily_demand_aep = df_aep.groupby(df_aep['ds'].dt.date)['y'].sum().reset_index()\n",
    "date_start = pd.to_datetime('2004-10-01 00:00:00')\n",
    "date_end = pd.to_datetime('2008-10-01 00:00:00')\n",
    "\n",
    "results_prophet = []\n",
    "i = 1\n",
    "\n",
    "while True:\n",
    "\n",
    "    print(f\"fit model {date_end + pd.Timedelta(days=1)}\")\n",
    "    result = prophet_based.forecast_next_day_hourly(df_aep, date_start, date_end, daily_demand_aep, manual = False, daily = True)\n",
    "    result['h_ahead'] = pd.to_datetime(result['ds']).dt.hour\n",
    "    results_prophet.append(result)\n",
    "\n",
    "    date_start += pd.Timedelta(days=1)\n",
    "    date_end += pd.Timedelta(days=1)\n",
    "    if i % 5 == 0:\n",
    "        results_prophet.to_csv('experiment_results/' + out_dir + '/results_prophet_dwy.csv', index=False)\n",
    "    if i == 70:\n",
    "        break\n",
    "    else:\n",
    "        i += 1\n",
    "\n",
    "results_prophet = pd.concat(results_prophet, ignore_index=True)\n",
    "\n",
    "results_prophet.to_csv('experiment_results/' + out_dir + '/results_prophet.csv', index=False)"
   ],
   "id": "8cd84b577cc13ea5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### test prophet based with daily, weekly, yearly seasonality",
   "id": "88e6f85b9cc64c1a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "daily_demand_aep = df_aep.groupby(df_aep['ds'].dt.date)['y'].sum().reset_index()\n",
    "date_start = pd.to_datetime('2004-10-01 00:00:00')\n",
    "date_end = pd.to_datetime('2008-10-01 00:00:00')\n",
    "\n",
    "results_prophet_dwy = []\n",
    "i = 1\n",
    "\n",
    "while True:\n",
    "\n",
    "    print(f\"fit model {date_end + pd.Timedelta(days=1)}\")\n",
    "    result = prophet_based.forecast_next_day_hourly(df_aep, date_start, date_end, daily_demand_aep, manual = False, daily = True, weekly = True, yearly = True)\n",
    "    result['h_ahead'] = pd.to_datetime(result['ds']).dt.hour\n",
    "    results_prophet_dwy.append(result)\n",
    "\n",
    "    date_start += pd.Timedelta(days=1)\n",
    "    date_end += pd.Timedelta(days=1)\n",
    "    if i % 5 == 0:\n",
    "        results_prophet_dwy.to_csv('experiment_results/' + out_dir + '/results_prophet_dwy.csv', index=False)\n",
    "    if i == 70:\n",
    "        break\n",
    "    else:\n",
    "        i += 1\n",
    "\n",
    "results_prophet_dwy = pd.concat(results_prophet_dwy, ignore_index=True)\n",
    "\n",
    "results_prophet_dwy.to_csv('experiment_results/' + out_dir + '/results_prophet_dwy.csv', index=False)"
   ],
   "id": "55aec6877637d97a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### test Simple RNN",
   "id": "23089dd82c59ca7c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-31T00:34:27.231977Z",
     "start_time": "2025-10-31T00:34:10.303014Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import RNN_simple\n",
    "train_config = RNN_simple.training_config(n_epochs = 30, device = torch.device(\"mps\"))\n",
    "results_simple = pd.DataFrame({'day': [], 'hour': [], 'y_hat': [], 'y':[]})\n",
    "date_start = pd.to_datetime('2004-10-01 00:00:00')\n",
    "date_end = pd.to_datetime('2008-10-01 00:00:00')\n",
    "\n",
    "i = 1\n",
    "\n",
    "while True:\n",
    "    print(i)\n",
    "    df_reduced = df_aep.loc[(df_aep['ds'] >= date_start) & (df_aep['ds'] < date_end)].copy()\n",
    "    model = RNN_simple.RNN_simple(latent_dim=32, activation='relu')\n",
    "    trainer = RNN_simple.RNN_train_simple(model, train_config)\n",
    "    forcast, true = trainer(df_reduced)\n",
    "\n",
    "    result = pd.DataFrame({'day': df_reduced['ds'].dt.date.max(), 'hour': list(range(24)), 'y_hat': forcast, 'y': true})\n",
    "\n",
    "    results_simple = pd.concat([results_simple, result]).reset_index(drop=True)\n",
    "\n",
    "    date_start += pd.Timedelta(days=1)\n",
    "    date_end += pd.Timedelta(days=1)\n",
    "    if i % 5 == 0:\n",
    "        results_simple.to_csv('experiment_results/' + out_dir + '/results_simple.csv', index=False)\n",
    "    if i == 70:\n",
    "        print('finish')\n",
    "        break\n",
    "    else:\n",
    "        i += 1\n",
    "\n",
    "results_simple.to_csv('experiment_results/' + out_dir + '/results_simple.csv', index=False)"
   ],
   "id": "2e3bd59d5a1cacd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "epoch 1 loss: 0.9297\n",
      "epoch 2 loss: 0.7866\n",
      "epoch 3 loss: 0.8256\n",
      "epoch 4 loss: 0.5188\n",
      "epoch 5 loss: 0.4083\n",
      "epoch 6 loss: 0.3761\n",
      "epoch 7 loss: 0.3105\n",
      "epoch 8 loss: 0.2558\n",
      "epoch 9 loss: 0.2898\n",
      "epoch 10 loss: 0.2545\n",
      "epoch 11 loss: 0.2815\n",
      "epoch 12 loss: 0.2581\n",
      "epoch 13 loss: 0.2439\n",
      "epoch 14 loss: 0.2198\n",
      "epoch 15 loss: 0.1981\n",
      "epoch 16 loss: 0.2495\n",
      "epoch 17 loss: 0.2034\n",
      "epoch 18 loss: 0.1860\n",
      "epoch 19 loss: 0.2124\n",
      "epoch 20 loss: 0.2385\n",
      "epoch 21 loss: 0.2146\n",
      "epoch 22 loss: 0.2008\n",
      "epoch 23 loss: 0.2310\n",
      "epoch 24 loss: 0.1922\n",
      "epoch 25 loss: 0.1880\n",
      "epoch 26 loss: 0.2117\n",
      "epoch 27 loss: 0.2231\n",
      "epoch 28 loss: 0.2038\n",
      "epoch 29 loss: 0.2179\n",
      "epoch 30 loss: 0.2196\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3h/5yfj7yqx7lg98t310bfxxr3m0000gn/T/ipykernel_68325/2114653971.py:18: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results_simple = pd.concat([results_simple, result]).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 loss: 1.0075\n",
      "epoch 2 loss: 0.7641\n",
      "epoch 3 loss: 0.8551\n",
      "epoch 4 loss: 0.5074\n",
      "epoch 5 loss: 0.4168\n",
      "epoch 6 loss: 0.3796\n",
      "epoch 7 loss: 0.3002\n",
      "epoch 8 loss: 0.3097\n",
      "epoch 9 loss: 0.2419\n",
      "epoch 10 loss: 0.2704\n",
      "epoch 11 loss: 0.2344\n",
      "epoch 12 loss: 0.2674\n",
      "epoch 13 loss: 0.1984\n",
      "epoch 14 loss: 0.2223\n",
      "epoch 15 loss: 0.2226\n",
      "epoch 16 loss: 0.2267\n",
      "epoch 17 loss: 0.2470\n",
      "epoch 18 loss: 0.2265\n",
      "epoch 19 loss: 0.2265\n",
      "epoch 20 loss: 0.2020\n",
      "epoch 21 loss: 0.2151\n",
      "epoch 22 loss: 0.2063\n",
      "epoch 23 loss: 0.2077\n",
      "epoch 24 loss: 0.2108\n",
      "epoch 25 loss: 0.2031\n",
      "epoch 26 loss: 0.1726\n",
      "epoch 27 loss: 0.2117\n",
      "epoch 28 loss: 0.1804\n",
      "epoch 29 loss: 0.2415\n",
      "epoch 30 loss: 0.1927\n",
      "3\n",
      "epoch 1 loss: 0.8549\n",
      "epoch 2 loss: 0.8623\n",
      "epoch 3 loss: 0.5929\n",
      "epoch 4 loss: 0.4481\n",
      "epoch 5 loss: 0.3882\n",
      "epoch 6 loss: 0.3611\n",
      "epoch 7 loss: 0.3067\n",
      "epoch 8 loss: 0.2933\n",
      "epoch 9 loss: 0.2610\n",
      "epoch 10 loss: 0.2796\n",
      "epoch 11 loss: 0.2418\n",
      "epoch 12 loss: 0.2531\n",
      "epoch 13 loss: 0.2410\n",
      "epoch 14 loss: 0.2528\n",
      "epoch 15 loss: 0.2112\n",
      "epoch 16 loss: 0.2483\n",
      "epoch 17 loss: 0.2097\n",
      "epoch 18 loss: 0.1969\n",
      "epoch 19 loss: 0.2082\n",
      "epoch 20 loss: 0.2160\n",
      "epoch 21 loss: 0.1810\n",
      "epoch 22 loss: 0.1878\n",
      "epoch 23 loss: 0.2236\n",
      "epoch 24 loss: 0.2271\n",
      "epoch 25 loss: 0.2046\n",
      "epoch 26 loss: 0.2382\n",
      "epoch 27 loss: 0.2050\n",
      "epoch 28 loss: 0.1669\n",
      "epoch 29 loss: 0.2276\n",
      "epoch 30 loss: 0.2077\n",
      "4\n",
      "epoch 1 loss: 0.9722\n",
      "epoch 2 loss: 0.8873\n",
      "epoch 3 loss: 0.6012\n",
      "epoch 4 loss: 0.4468\n",
      "epoch 5 loss: 0.4218\n",
      "epoch 6 loss: 0.3294\n",
      "epoch 7 loss: 0.2704\n",
      "epoch 8 loss: 0.2874\n",
      "epoch 9 loss: 0.2335\n",
      "epoch 10 loss: 0.2885\n",
      "epoch 11 loss: 0.2294\n",
      "epoch 12 loss: 0.3097\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 14\u001B[0m\n\u001B[1;32m     12\u001B[0m model \u001B[38;5;241m=\u001B[39m RNN_simple\u001B[38;5;241m.\u001B[39mRNN_simple(latent_dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m32\u001B[39m, activation\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrelu\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     13\u001B[0m trainer \u001B[38;5;241m=\u001B[39m RNN_simple\u001B[38;5;241m.\u001B[39mRNN_train_simple(model, train_config)\n\u001B[0;32m---> 14\u001B[0m forcast, true \u001B[38;5;241m=\u001B[39m \u001B[43mtrainer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf_reduced\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     16\u001B[0m result \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame({\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mday\u001B[39m\u001B[38;5;124m'\u001B[39m: df_reduced[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mds\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mdt\u001B[38;5;241m.\u001B[39mdate\u001B[38;5;241m.\u001B[39mmax(), \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhour\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m24\u001B[39m)), \u001B[38;5;124m'\u001B[39m\u001B[38;5;124my_hat\u001B[39m\u001B[38;5;124m'\u001B[39m: forcast, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124my\u001B[39m\u001B[38;5;124m'\u001B[39m: true})\n\u001B[1;32m     18\u001B[0m results_simple \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mconcat([results_simple, result])\u001B[38;5;241m.\u001B[39mreset_index(drop\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[0;32m~/Documents/GitHub/RNN_based_downscaling/RNN_simple.py:267\u001B[0m, in \u001B[0;36mRNN_train_simple.__call__\u001B[0;34m(self, df)\u001B[0m\n\u001B[1;32m    265\u001B[0m o, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel(xb)\n\u001B[1;32m    266\u001B[0m loss \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mmse_loss(o, yb)\n\u001B[0;32m--> 267\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    268\u001B[0m torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mclip_grad_norm_(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mparameters(), \u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m    269\u001B[0m opt\u001B[38;5;241m.\u001B[39mstep()\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/_tensor.py:647\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    637\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    638\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    639\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    640\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    645\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    646\u001B[0m     )\n\u001B[0;32m--> 647\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    648\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    649\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/autograd/__init__.py:354\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    349\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    351\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    352\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    353\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 354\u001B[0m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    355\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    356\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    357\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    358\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    359\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_tuple\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    360\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    361\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    362\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/autograd/graph.py:829\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[0;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[1;32m    827\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[1;32m    828\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 829\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    830\u001B[0m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    831\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[1;32m    832\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    833\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "6d8c4c83-0604-4c0c-8fd5-b696b180310f",
   "metadata": {},
   "source": [
    "### test RNN+self convolution"
   ]
  },
  {
   "cell_type": "code",
   "id": "6007d956-d042-4f55-b110-4649eb5b23d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-31T00:35:21.756606Z",
     "start_time": "2025-10-31T00:34:50.091217Z"
    }
   },
   "source": [
    "import RNN_attention\n",
    "train_config = RNN_attention.training_config(n_epochs = 30, device = torch.device(\"mps\"))\n",
    "results_attention = pd.DataFrame({'day': [], 'hour': [], 'y_hat': [], 'y':[]})\n",
    "date_start = pd.to_datetime('2004-10-01 00:00:00')\n",
    "date_end = pd.to_datetime('2008-10-01 00:00:00')\n",
    "\n",
    "i = 1\n",
    "\n",
    "while True:\n",
    "    print(i)\n",
    "    df_reduced = df_aep.loc[(df_aep['ds'] >= date_start) & (df_aep['ds'] < date_end)].copy()\n",
    "    model = RNN_attention.RNN_attention(latent_dim=32, d_model=128, nhead=4)\n",
    "    trainer = RNN_attention.RNN_train_attention(model, train_config)\n",
    "    forcast, true = trainer(df_reduced)\n",
    "    \n",
    "    result = pd.DataFrame({'day': df_reduced['ds'].dt.date.max(), 'hour': list(range(24)), 'y_hat': forcast, 'y': true})\n",
    "\n",
    "    results_attention = pd.concat([results_attention, result]).reset_index(drop=True)\n",
    "\n",
    "    date_start += pd.Timedelta(days=1)\n",
    "    date_end += pd.Timedelta(days=1)\n",
    "    if i % 5 == 0:\n",
    "        results_attention.to_csv('experiment_results/' + out_dir + '/results_attention.csv', index=False)\n",
    "    if i == 70:\n",
    "        print('finish')\n",
    "        break\n",
    "    else:\n",
    "        i += 1\n",
    "\n",
    "results_attention.to_csv('experiment_results/' + out_dir + '/results_attention.csv', index=False)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "epoch 1 loss: 0.4665\n",
      "epoch 2 loss: 0.2916\n",
      "epoch 3 loss: 0.2224\n",
      "epoch 4 loss: 0.2327\n",
      "epoch 5 loss: 0.2090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3h/5yfj7yqx7lg98t310bfxxr3m0000gn/T/ipykernel_68325/913880509.py:18: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results_attention = pd.concat([results_attention, result]).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "epoch 1 loss: 0.3767\n",
      "epoch 2 loss: 0.2292\n",
      "epoch 3 loss: 0.2153\n",
      "epoch 4 loss: 0.2090\n",
      "epoch 5 loss: 0.2543\n",
      "3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 14\u001B[0m\n\u001B[1;32m     12\u001B[0m model \u001B[38;5;241m=\u001B[39m RNN_attention\u001B[38;5;241m.\u001B[39mRNN_attention(latent_dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m32\u001B[39m, d_model\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m128\u001B[39m, nhead\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m4\u001B[39m)\n\u001B[1;32m     13\u001B[0m trainer \u001B[38;5;241m=\u001B[39m RNN_attention\u001B[38;5;241m.\u001B[39mRNN_train_attention(model, train_config)\n\u001B[0;32m---> 14\u001B[0m forcast, true \u001B[38;5;241m=\u001B[39m \u001B[43mtrainer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf_reduced\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     16\u001B[0m result \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame({\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mday\u001B[39m\u001B[38;5;124m'\u001B[39m: df_reduced[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mds\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mdt\u001B[38;5;241m.\u001B[39mdate\u001B[38;5;241m.\u001B[39mmax(), \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhour\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m24\u001B[39m)), \u001B[38;5;124m'\u001B[39m\u001B[38;5;124my_hat\u001B[39m\u001B[38;5;124m'\u001B[39m: forcast, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124my\u001B[39m\u001B[38;5;124m'\u001B[39m: true})\n\u001B[1;32m     18\u001B[0m results_attention \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mconcat([results_attention, result])\u001B[38;5;241m.\u001B[39mreset_index(drop\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[0;32m~/Documents/GitHub/RNN_based_downscaling/RNN_attention.py:326\u001B[0m, in \u001B[0;36mRNN_train_attention.__call__\u001B[0;34m(self, df)\u001B[0m\n\u001B[1;32m    324\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m xb, yb \u001B[38;5;129;01min\u001B[39;00m loader:\n\u001B[0;32m--> 326\u001B[0m     xb, yb \u001B[38;5;241m=\u001B[39m \u001B[43mxb\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m, yb\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining_config\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m    327\u001B[0m     opt\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m    328\u001B[0m     o, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel(xb)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "448d1d3a-c3c8-46e9-bad0-55eb3eb3457c",
   "metadata": {},
   "source": "### test RNN+self convolution + fourier matrix (weekly, monthly, yearly)"
  },
  {
   "cell_type": "code",
   "id": "64c5838c-0d23-48b1-b71d-2c0c79c15634",
   "metadata": {},
   "source": [
    "import RNN_fourier_RNN\n",
    "\n",
    "train_config = RNN_fourier_RNN.training_config(n_epochs=30, device=torch.device(\"mps\"))\n",
    "fourier_conf = RNN_fourier_RNN.fourier_config(mode=\"matrix\", K_weekly=3, K_monthly=6, K_yearly=10)\n",
    "\n",
    "# Calculate dimensions correctly\n",
    "K_total = fourier_conf.K_weekly + fourier_conf.K_monthly + fourier_conf.K_yearly\n",
    "F_per_hour = 2 * K_total  # sin/cos pairs per hour\n",
    "\n",
    "if fourier_conf.mode == \"vector\":\n",
    "    cont_dim = 1 + F_per_hour\n",
    "    fourier_dim = F_per_hour\n",
    "else:  # matrix mode\n",
    "    cont_dim = 1 + 24 * F_per_hour  # Each hour gets F_per_hour features\n",
    "    fourier_dim = F_per_hour\n",
    "\n",
    "results_fourier = pd.DataFrame({'day': [], 'hour': [], 'y_hat': [], 'y':[]})\n",
    "date_start = pd.to_datetime('2004-10-01 00:00:00')\n",
    "date_end = pd.to_datetime('2008-10-01 00:00:00')\n",
    "\n",
    "i = 1\n",
    "\n",
    "while True:\n",
    "    print(i)\n",
    "    df_reduced = df_aep.loc[(df_aep['ds'] >= date_start) & (df_aep['ds'] < date_end)].copy()\n",
    "    model = RNN_fourier_RNN.RNN_fourier(cont_dim=cont_dim, fourier_dim=fourier_dim, xf_mode=\"matrix\",\n",
    "                                               d_model=128,latent_dim=32,nhead=4)\n",
    "    trainer = RNN_fourier_RNN.RNN_train_fourier(model, train_config, fourier_conf)\n",
    "    forcast, true = trainer(df_reduced)\n",
    "\n",
    "    result = pd.DataFrame({'day': df_reduced['ds'].dt.date.max(), 'hour': list(range(24)), 'y_hat': forcast, 'y': true})\n",
    "\n",
    "    results_fourier = pd.concat([results_fourier, result]).reset_index(drop=True)\n",
    "\n",
    "    date_start += pd.Timedelta(days=1)\n",
    "    date_end += pd.Timedelta(days=1)\n",
    "    if i % 5 == 0:\n",
    "        results_fourier.to_csv('experiment_results/' + out_dir + '/results_fourier.csv', index=False)\n",
    "    if i == 70:\n",
    "        print('finish')\n",
    "        break\n",
    "    else:\n",
    "        i += 1\n",
    "\n",
    "results_fourier.to_csv('experiment_results/' + out_dir + '/results_fourier.csv', index=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "34d07cb93e10a743"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "k = 24*3\n",
    "x_ax = np.arange(k)\n",
    "\n",
    "plt.plot(x_ax, results_fourier['y_hat'][:k], marker=\"o\", linestyle=\"-\", color=\"b\")\n",
    "plt.plot(x_ax, results_fourier['y'][:k], marker=\"o\", linestyle=\"-\", color=\"r\")\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"y_Tplus1\")\n",
    "plt.title(\"Line plot of y_Tplus1\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "id": "417e3cb139fad504",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "547af856-e054-4279-96e4-e1e4264db04d",
   "metadata": {},
   "source": [
    "## Comed"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from prophet import Prophet\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import model_summary as model_class\n",
    "import prophet_linear_adjust as prophet_based\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "id": "498db8ff6ea6c4cc",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8da89f60-d42c-4b68-b83f-b10f930f69c1",
   "metadata": {},
   "source": [
    "df_comed = pd.read_csv(\"COMED_hourly.csv\")\n",
    "df_comed.rename(columns={'Datetime': 'ds', 'COMED_MW': 'y'}, inplace=True)\n",
    "df_comed['ds'] = pd.to_datetime(df_comed['ds'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "out_dir = 'Comed_results'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### test prophet based without seasonality",
   "id": "a72919bd420eae26"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "daily_demand_comed = df_comed.groupby(df_comed['ds'].dt.date)['y'].sum().reset_index()\n",
    "date_start = pd.to_datetime('2012-10-01 00:00:00')\n",
    "date_end = pd.to_datetime('2016-10-01 00:00:00')\n",
    "\n",
    "results_prophet = []\n",
    "i = 1\n",
    "\n",
    "while True:\n",
    "\n",
    "    print(f\"fit model {date_end + pd.Timedelta(days=1)}\")\n",
    "    result = prophet_based.forecast_next_day_hourly(df_comed, date_start, date_end, daily_demand_comed, manual = False, daily = True)\n",
    "    result['h_ahead'] = pd.to_datetime(result['ds']).dt.hour\n",
    "    results_prophet.append(result)\n",
    "\n",
    "    date_start += pd.Timedelta(days=1)\n",
    "    date_end += pd.Timedelta(days=1)\n",
    "\n",
    "    if i == 70:\n",
    "        break\n",
    "    else:\n",
    "        i += 1\n",
    "\n",
    "results_prophet = pd.concat(results_prophet, ignore_index=True)\n",
    "\n",
    "results_prophet.to_csv(out_dir + '/results_prophet.csv', index=False)"
   ],
   "id": "68fe5f9ff5d05326",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### test prophet based with weekly seasonality",
   "id": "6fa4ad7e711b555b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "daily_demand_comed = df_comed.groupby(df_comed['ds'].dt.date)['y'].sum().reset_index()\n",
    "date_start = pd.to_datetime('2012-10-01 00:00:00')\n",
    "date_end = pd.to_datetime('2016-10-01 00:00:00')\n",
    "\n",
    "results_prophet_w = []\n",
    "i = 1\n",
    "\n",
    "while True:\n",
    "\n",
    "    print(f\"fit model {date_end + pd.Timedelta(days=1)}\")\n",
    "    result = prophet_based.forecast_next_day_hourly(df_comed, date_start, date_end, daily_demand_comed, manual = False, daily = True, weekly = True)\n",
    "    result['h_ahead'] = pd.to_datetime(result['ds']).dt.hour\n",
    "    results_prophet_w.append(result)\n",
    "\n",
    "    date_start += pd.Timedelta(days=1)\n",
    "    date_end += pd.Timedelta(days=1)\n",
    "\n",
    "    if i == 70:\n",
    "        break\n",
    "    else:\n",
    "        i += 1\n",
    "\n",
    "results_prophet_w = pd.concat(results_prophet_w, ignore_index=True)\n",
    "\n",
    "results_prophet_w.to_csv(out_dir + '/results_prophet_w.csv', index=False)"
   ],
   "id": "56304448d5c902b5",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f9ee246e-5e4c-45dc-b92d-c225f8cc6dd5",
   "metadata": {},
   "source": [
    "### test RNN+self convolution"
   ]
  },
  {
   "cell_type": "code",
   "id": "a9fbb12a-f16f-4437-acbf-e9ba9bdc7a2a",
   "metadata": {},
   "source": [
    "train_config = model_class.training_config(n_epochs = 30, device = torch.device(\"mps\"))\n",
    "results_aep_self_con = pd.DataFrame({'day': [], 'hour': [], 'y_hat': [], 'y':[]})\n",
    "date_start = pd.to_datetime('2012-10-01 00:00:00')\n",
    "date_end = pd.to_datetime('2016-10-01 00:00:00')\n",
    "\n",
    "i = 1\n",
    "\n",
    "while True:\n",
    "    print(i)\n",
    "    df_reduced = df_comed.loc[(df_comed['ds'] >= date_start) & (df_comed['ds'] < date_end)].copy()\n",
    "    model = model_class.RNN_FeatureAttention()\n",
    "    trainer = model_class.RNN_train_1(model, train_config)\n",
    "    forcast, true = trainer(df_reduced)\n",
    "    \n",
    "    result = pd.DataFrame({'day': df_reduced['ds'].dt.date.max(), 'hour': list(range(24)), 'y_hat': forcast, 'y': true})\n",
    "\n",
    "    results_aep_self_con = pd.concat([results_aep_self_con, result]).reset_index(drop=True)\n",
    "\n",
    "    date_start += pd.Timedelta(days=1)\n",
    "    date_end += pd.Timedelta(days=1)\n",
    "    if i == 70:\n",
    "        print('finish')\n",
    "        break\n",
    "    else:\n",
    "        i += 1\n",
    "\n",
    "results_aep_self_con.to_csv(out_dir + '/results_aep_self_con.csv', index=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "098899fc-f69c-40fd-97d5-21d4a8e72619",
   "metadata": {},
   "source": [
    "### test RNN+self convolution + numerical weekday"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_config = model_class.training_config(n_epochs = 30, device = torch.device(\"mps\"))\n",
    "results_aep_self_con_wn = pd.DataFrame({'day': [], 'hour': [], 'y_hat': [], 'y':[]})\n",
    "date_start = pd.to_datetime('2012-10-01 00:00:00')\n",
    "date_end = pd.to_datetime('2016-10-01 00:00:00')\n",
    "\n",
    "i = 1\n",
    "\n",
    "while True:\n",
    "    print(i)\n",
    "    df_reduced = df_comed.loc[(df_comed['ds'] >= date_start) & (df_comed['ds'] < date_end)].copy()\n",
    "    model = model_class.RNN_FeatureAttention_wn()\n",
    "    trainer = model_class.RNN_train_1(model, train_config)\n",
    "    forcast, true = trainer(df_reduced)\n",
    "\n",
    "    result = pd.DataFrame({'day': df_reduced['ds'].dt.date.max(), 'hour': list(range(24)), 'y_hat': forcast, 'y': true})\n",
    "\n",
    "    results_aep_self_con_wn = pd.concat([results_aep_self_con_wn, result]).reset_index(drop=True)\n",
    "\n",
    "    date_start += pd.Timedelta(days=1)\n",
    "    date_end += pd.Timedelta(days=1)\n",
    "    if i == 70:\n",
    "        print('finish')\n",
    "        break\n",
    "    else:\n",
    "        i += 1\n",
    "\n",
    "results_aep_self_con_wn.to_csv(out_dir + '/results_aep_self_con_wn.csv', index=False)"
   ],
   "id": "f6b8f3248cc64895",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f21f0b6e-940d-4aba-ade0-11c129bfcf90",
   "metadata": {},
   "source": [
    "### test RNN+self convolution + categorical weekday"
   ]
  },
  {
   "cell_type": "code",
   "id": "6725e474-71bd-4c89-9874-c86e0542477d",
   "metadata": {},
   "source": [
    "train_config = model_class.training_config(n_epochs = 30, device = torch.device(\"mps\"))\n",
    "results_aep_self_con_wc = pd.DataFrame({'day': [], 'hour': [], 'y_hat': [], 'y':[]})\n",
    "date_start = pd.to_datetime('2012-10-01 00:00:00')\n",
    "date_end = pd.to_datetime('2016-10-01 00:00:00')\n",
    "\n",
    "i = 1\n",
    "\n",
    "while True:\n",
    "    print(i)\n",
    "    df_reduced = df_comed.loc[(df_comed['ds'] >= date_start) & (df_comed['ds'] < date_end)].copy()\n",
    "    model = model_class.RNN_FeatureAttention_wc()\n",
    "    trainer = model_class.RNN_train_2(model, train_config)\n",
    "    forcast, true = trainer(df_reduced)\n",
    "    \n",
    "    result = pd.DataFrame({'day': df_reduced['ds'].dt.date.max(), 'hour': list(range(24)), 'y_hat': forcast, 'y': true})\n",
    "\n",
    "    results_aep_self_con_wc = pd.concat([results_aep_self_con_wc, result]).reset_index(drop=True)\n",
    "\n",
    "    date_start += pd.Timedelta(days=1)\n",
    "    date_end += pd.Timedelta(days=1)\n",
    "    if i == 70:\n",
    "        print('finish')\n",
    "        break\n",
    "    else:\n",
    "        i += 1\n",
    "results_aep_self_con_wc.to_csv(out_dir + '/results_aep_self_con_wc.csv', index=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5213cfe5-3065-40c2-8f0a-c36d9e0d7380",
   "metadata": {},
   "source": [
    "### test RNN+self convolution + fourier vector (only weekly)"
   ]
  },
  {
   "cell_type": "code",
   "id": "57c81264-58cc-4abe-b8ae-3dcc4ecd8e02",
   "metadata": {},
   "source": [
    "train_config = model_class.training_config(n_epochs = 30, device = torch.device(\"mps\"))\n",
    "fourier_conf = model_class.fourier_config(mode=\"vector\", K_weekly=3, K_monthly=0)\n",
    "F_pairs = 2*(fourier_conf.K_weekly + fourier_conf.K_monthly)\n",
    "if fourier_conf.mode == \"vector\":\n",
    "    cont_dim = 1 + F_pairs\n",
    "    fourier_dim = F_pairs\n",
    "else:\n",
    "    cont_dim = 1 + 24*F_pairs\n",
    "    fourier_dim = F_pairs\n",
    "            \n",
    "results_aep_fourier_w = pd.DataFrame({'day': [], 'hour': [], 'y_hat': [], 'y':[]})\n",
    "date_start = pd.to_datetime('2012-10-01 00:00:00')\n",
    "date_end = pd.to_datetime('2016-10-01 00:00:00')\n",
    "\n",
    "i = 1\n",
    "\n",
    "while True:\n",
    "    print(i)\n",
    "    df_reduced = df_comed.loc[(df_comed['ds'] >= date_start) & (df_comed['ds'] < date_end)].copy()\n",
    "    model = model_class.RNN_fourier(cont_dim=cont_dim, fourier_dim=fourier_dim,\n",
    "                                    xf_mode=fourier_conf.mode, d_model=128, nhead=4, \n",
    "                                    activation=\"relu\", learn_z0=True, dropout=0.0,\n",
    "                                    H=24, use_gate=True, nonneg_U0=False)\n",
    "    trainer = model_class.RNN_train_fourier(model, train_config, fourier_conf)\n",
    "    forcast, true = trainer(df_reduced)\n",
    "    \n",
    "    result = pd.DataFrame({'day': df_reduced['ds'].dt.date.max(), 'hour': list(range(24)), 'y_hat': forcast, 'y': true})\n",
    "\n",
    "    results_aep_fourier_w = pd.concat([results_aep_fourier_w, result]).reset_index(drop=True)\n",
    "\n",
    "    date_start += pd.Timedelta(days=1)\n",
    "    date_end += pd.Timedelta(days=1)\n",
    "    if i == 70:\n",
    "        print('finish')\n",
    "        break\n",
    "    else:\n",
    "        i += 1\n",
    "\n",
    "results_aep_fourier_w.to_csv(out_dir + '/results_aep_fourier_w.csv', index=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "30cf4e2e-c31d-4a79-991a-8749f4801426",
   "metadata": {},
   "source": [
    "### test RNN+self convolution + fourier matrix (only weekly)"
   ]
  },
  {
   "cell_type": "code",
   "id": "ee279949-a82f-452f-901e-03c15a84f6dd",
   "metadata": {},
   "source": [
    "train_config = model_class.training_config(n_epochs = 30, device = torch.device(\"mps\"))\n",
    "fourier_conf = model_class.fourier_config(mode=\"matrix\", K_weekly=3, K_monthly=0)\n",
    "F_pairs = 2*(fourier_conf.K_weekly + fourier_conf.K_monthly)\n",
    "if fourier_conf.mode == \"vector\":\n",
    "    cont_dim = 1 + F_pairs\n",
    "    fourier_dim = F_pairs\n",
    "else:\n",
    "    cont_dim = 1 + 24*F_pairs\n",
    "    fourier_dim = F_pairs\n",
    "            \n",
    "results_aep_fourier_w_matrix = pd.DataFrame({'day': [], 'hour': [], 'y_hat': [], 'y':[]})\n",
    "date_start = pd.to_datetime('2012-10-01 00:00:00')\n",
    "date_end = pd.to_datetime('2016-10-01 00:00:00')\n",
    "\n",
    "i = 1\n",
    "\n",
    "while True:\n",
    "    print(i)\n",
    "    df_reduced = df_comed.loc[(df_comed['ds'] >= date_start) & (df_comed['ds'] < date_end)].copy()\n",
    "    model = model_class.RNN_fourier(cont_dim=cont_dim, fourier_dim=fourier_dim,\n",
    "                                    xf_mode=fourier_conf.mode, d_model=128, nhead=4, \n",
    "                                    activation=\"relu\", learn_z0=True, dropout=0.0,\n",
    "                                    H=24, use_gate=True, nonneg_U0=False)\n",
    "    trainer = model_class.RNN_train_fourier(model, train_config, fourier_conf)\n",
    "    forcast, true = trainer(df_reduced)\n",
    "    \n",
    "    result = pd.DataFrame({'day': df_reduced['ds'].dt.date.max(), 'hour': list(range(24)), 'y_hat': forcast, 'y': true})\n",
    "\n",
    "    results_aep_fourier_w_matrix = pd.concat([results_aep_fourier_w_matrix, result]).reset_index(drop=True)\n",
    "\n",
    "    date_start += pd.Timedelta(days=1)\n",
    "    date_end += pd.Timedelta(days=1)\n",
    "    if i == 70:\n",
    "        print('finish')\n",
    "        break\n",
    "    else:\n",
    "        i += 1\n",
    "\n",
    "results_aep_fourier_w_matrix.to_csv(out_dir + '/results_aep_fourier_w_matrix.csv', index=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_config = model_class.training_config(n_epochs = 30, device = torch.device(\"mps\"))\n",
    "fourier_conf = model_class.fourier_config(mode=\"matrix\", K_weekly=3, K_monthly=6, K_yearly=10)\n",
    "F_pairs = 2*(fourier_conf.K_weekly + fourier_conf.K_monthly + fourier_conf.K_yearly)\n",
    "if fourier_conf.mode == \"vector\":\n",
    "    cont_dim = 1 + F_pairs\n",
    "    fourier_dim = F_pairs\n",
    "else:\n",
    "    cont_dim = 1 + 24*F_pairs\n",
    "    fourier_dim = F_pairs\n",
    "\n",
    "results_aep_fourier_w_matrix = pd.DataFrame({'day': [], 'hour': [], 'y_hat': [], 'y':[]})\n",
    "date_start = pd.to_datetime('2012-10-01 00:00:00')\n",
    "date_end = pd.to_datetime('2016-10-01 00:00:00')\n",
    "\n",
    "i = 1\n",
    "\n",
    "while True:\n",
    "    print(i)\n",
    "    df_reduced = df_comed.loc[(df_comed['ds'] >= date_start) & (df_comed['ds'] < date_end)].copy()\n",
    "    model = model_class.RNN_fourier(cont_dim=cont_dim, fourier_dim=fourier_dim,\n",
    "                                    xf_mode=fourier_conf.mode, d_model=128, nhead=4,\n",
    "                                    activation=\"relu\", learn_z0=True, dropout=0.0,\n",
    "                                    H=24, use_gate=True, nonneg_U0=False)\n",
    "    trainer = model_class.RNN_train_fourier(model, train_config, fourier_conf)\n",
    "    forcast, true = trainer(df_reduced)\n",
    "\n",
    "    result = pd.DataFrame({'day': df_reduced['ds'].dt.date.max(), 'hour': list(range(24)), 'y_hat': forcast, 'y': true})\n",
    "\n",
    "    results_aep_fourier_w_matrix = pd.concat([results_aep_fourier_w_matrix, result]).reset_index(drop=True)\n",
    "\n",
    "    date_start += pd.Timedelta(days=1)\n",
    "    date_end += pd.Timedelta(days=1)\n",
    "    if i == 70:\n",
    "        print('finish')\n",
    "        break\n",
    "    else:\n",
    "        i += 1\n",
    "\n",
    "results_aep_fourier_w_matrix.to_csv(out_dir + '/results_aep_fourier_kw3_km6_ky10_matrix.csv', index=False)"
   ],
   "id": "946c3d2aa7d3f8be",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### test RNN_based Asym",
   "id": "b23fd78f03c3bfc0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_config = model_class.training_config(n_epochs = 30, device = torch.device(\"mps\"))\n",
    "results_aep_asy_con = pd.DataFrame({'day': [], 'hour': [], 'y_hat': [], 'y':[]})\n",
    "date_start = pd.to_datetime('2012-10-01 00:00:00')\n",
    "date_end = pd.to_datetime('2016-10-01 00:00:00')\n",
    "\n",
    "i = 1\n",
    "\n",
    "while True:\n",
    "    print(i)\n",
    "    df_reduced = df_comed.loc[(df_comed['ds'] >= date_start) & (df_comed['ds'] < date_end)].copy()\n",
    "    model = model_class.RNN_asym_feature()\n",
    "    trainer = model_class.RNN_train_1(model, train_config)\n",
    "    forcast, true = trainer(df_reduced)\n",
    "\n",
    "    result = pd.DataFrame({'day': df_reduced['ds'].dt.date.max(), 'hour': list(range(24)), 'y_hat': forcast, 'y': true})\n",
    "\n",
    "    results_aep_asy_con = pd.concat([results_aep_asy_con, result]).reset_index(drop=True)\n",
    "\n",
    "    date_start += pd.Timedelta(days=1)\n",
    "    date_end += pd.Timedelta(days=1)\n",
    "    if i == 70:\n",
    "        print('finish')\n",
    "        break\n",
    "    else:\n",
    "        i += 1\n",
    "\n",
    "results_aep_asy_con.to_csv(out_dir + '/results_aep_asy_con.csv', index=False)"
   ],
   "id": "44d660bd30d194a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "out_dir",
   "id": "264e1975114e7808",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Test RNN_based ARMA",
   "id": "ad6bccf216b7bf43"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_config = model_class.training_config(n_epochs = 30, device = torch.device(\"cpu\"))\n",
    "results_arma_asy_con = pd.DataFrame({'day': [], 'hour': [], 'y_hat': [], 'y':[]})\n",
    "date_start = pd.to_datetime('2012-10-01 00:00:00')\n",
    "date_end = pd.to_datetime('2016-10-01 00:00:00')\n",
    "\n",
    "i = 1\n",
    "\n",
    "while True:\n",
    "    print(i)\n",
    "    df_reduced = df_comed.loc[(df_comed['ds'] >= date_start) & (df_comed['ds'] < date_end)].copy()\n",
    "    model = model_class.RNN_ARMA_Feature()\n",
    "    trainer = model_class.RNN_train_1(model, train_config)\n",
    "    forcast, true = trainer(df_reduced)\n",
    "    result = pd.DataFrame({'day': df_reduced['ds'].dt.date.max(), 'hour': list(range(24)), 'y_hat': forcast, 'y': true})\n",
    "\n",
    "    results_arma_asy_con = pd.concat([results_arma_asy_con, result]).reset_index(drop=True)\n",
    "\n",
    "    date_start += pd.Timedelta(days=1)\n",
    "    date_end += pd.Timedelta(days=1)\n",
    "    if i == 70:\n",
    "        print('finish')\n",
    "        break\n",
    "    else:\n",
    "        i += 1\n",
    "\n",
    "results_arma_asy_con.to_csv(out_dir + '/results_arma_asy_con.csv', index=False)"
   ],
   "id": "7ba75a50b7a82bbf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### test Dirich",
   "id": "f033d8578ca1b48b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import model_dich\n",
    "\n",
    "class Cfg: pass\n",
    "cfg = Cfg()\n",
    "cfg.device = \"mps\"\n",
    "cfg.n_epochs = 90\n",
    "cfg.lr = 5e-4\n",
    "cfg.x_col = \"x\"\n",
    "cfg.y_cols = [f\"y_{i}\" for i in range(24)]\n",
    "cfg.T_hist = 32\n",
    "cfg.batch_size = 64\n",
    "cfg.test_ratio = 0.2\n",
    "cfg.kl_coeff = 1.0\n",
    "\n",
    "\n",
    "train_config = model_class.training_config(n_epochs = 3, device = torch.device(\"cpu\"))\n",
    "results_prob_dich = pd.DataFrame({'day': [], 'hour': [], 'y_hat': [], 'y':[]})\n",
    "date_start = pd.to_datetime('2012-10-01 00:00:00')\n",
    "date_end = pd.to_datetime('2016-10-01 00:00:00')\n",
    "\n",
    "i = 1\n",
    "\n",
    "while True:\n",
    "    print(i)\n",
    "    df_reduced = df_comed.loc[(df_comed['ds'] >= date_start) & (df_comed['ds'] < date_end)].copy()\n",
    "    model = model_dich.DirichletComposerRNN(device=torch.device(\"cpu\"))\n",
    "    trainer = model_dich.RNN_train(model, cfg)\n",
    "    ret = trainer(df_reduced)\n",
    "    result = pd.DataFrame({'day': df_reduced['ds'].dt.date.max(), 'hour': list(range(24)), 'y_hat': ret['Y_pred'].flatten(), 'y': ret['Y_test'].flatten()})\n",
    "\n",
    "    results_prob_dich = pd.concat([results_prob_dich, result]).reset_index(drop=True)\n",
    "\n",
    "    date_start += pd.Timedelta(days=1)\n",
    "    date_end += pd.Timedelta(days=1)\n",
    "    if i == 70:\n",
    "        print('finish')\n",
    "        break\n",
    "    else:\n",
    "        i += 1\n",
    "\n",
    "results_prob_dich.to_csv(out_dir + '/results_prob_dich_90epoch.csv', index=False)"
   ],
   "id": "767da83a436cca0c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "out_dir",
   "id": "23d6231718bf88a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3ca0dee4-c216-4ab3-a0c6-602048f0dc4c",
   "metadata": {},
   "source": [
    "## DAYTON"
   ]
  },
  {
   "cell_type": "code",
   "id": "89b69783-5c4d-4ce5-9707-7b41ee140a4a",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from prophet import Prophet\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import model_summary as model_class\n",
    "import prophet_linear_adjust as prophet_based\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "301ff3fe-0ffc-46c8-bd19-98d31cd4be20",
   "metadata": {},
   "source": [
    "df_dayton = pd.read_csv(\"DAYTON_hourly.csv\")\n",
    "df_dayton.rename(columns={'Datetime': 'ds', 'DAYTON_MW': 'y'}, inplace=True)\n",
    "df_dayton['ds'] = pd.to_datetime(df_dayton['ds'], format='%Y-%m-%d %H:%M:%S')\n",
    "out_dir = 'Dayton_results'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### test prophet based without seasonality",
   "id": "c2a43fda96025ca4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "daily_demand_dayton = df_dayton.groupby(df_dayton['ds'].dt.date)['y'].sum().reset_index()\n",
    "date_start = pd.to_datetime('2012-10-01 00:00:00')\n",
    "date_end = pd.to_datetime('2016-10-01 00:00:00')\n",
    "\n",
    "results_prophet = []\n",
    "i = 1\n",
    "\n",
    "while True:\n",
    "\n",
    "    print(f\"fit model {date_end + pd.Timedelta(days=1)}\")\n",
    "    result = prophet_based.forecast_next_day_hourly(df_dayton, date_start, date_end, daily_demand_dayton, manual = False, daily = True)\n",
    "    result['h_ahead'] = pd.to_datetime(result['ds']).dt.hour\n",
    "    results_prophet.append(result)\n",
    "\n",
    "    date_start += pd.Timedelta(days=1)\n",
    "    date_end += pd.Timedelta(days=1)\n",
    "\n",
    "    if i == 70:\n",
    "        break\n",
    "    else:\n",
    "        i += 1\n",
    "\n",
    "results_prophet = pd.concat(results_prophet, ignore_index=True)\n",
    "\n",
    "results_prophet.to_csv(out_dir + '/results_prophet.csv', index=False)"
   ],
   "id": "50bf8437d421c5e2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### test prophet based with seasonality",
   "id": "87a4d70da45eb581"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "daily_demand_dayton = df_dayton.groupby(df_dayton['ds'].dt.date)['y'].sum().reset_index()\n",
    "date_start = pd.to_datetime('2012-10-01 00:00:00')\n",
    "date_end = pd.to_datetime('2016-10-01 00:00:00')\n",
    "\n",
    "results_prophet_w = []\n",
    "i = 1\n",
    "\n",
    "while True:\n",
    "\n",
    "    print(f\"fit model {date_end + pd.Timedelta(days=1)}\")\n",
    "    result = prophet_based.forecast_next_day_hourly(df_dayton, date_start, date_end, daily_demand_dayton, manual = False, daily = True, weekly = True)\n",
    "    result['h_ahead'] = pd.to_datetime(result['ds']).dt.hour\n",
    "    results_prophet_w.append(result)\n",
    "\n",
    "    date_start += pd.Timedelta(days=1)\n",
    "    date_end += pd.Timedelta(days=1)\n",
    "\n",
    "    if i == 70:\n",
    "        break\n",
    "    else:\n",
    "        i += 1\n",
    "\n",
    "results_prophet_w = pd.concat(results_prophet_w, ignore_index=True)\n",
    "\n",
    "results_prophet_w.to_csv(out_dir + '/results_prophet_w.csv', index=False)"
   ],
   "id": "49a41aceef093834",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "15e06fbb-b00d-4517-b3e3-b7b87c857a2b",
   "metadata": {},
   "source": [
    "### test RNN+self convolution"
   ]
  },
  {
   "cell_type": "code",
   "id": "9893c3dd-afac-42f1-97a0-c37c2a8f912c",
   "metadata": {},
   "source": [
    "train_config = model_class.training_config(n_epochs = 30, device = torch.device(\"mps\"))\n",
    "results_aep_self_con = pd.DataFrame({'day': [], 'hour': [], 'y_hat': [], 'y':[]})\n",
    "date_start = pd.to_datetime('2012-10-01 00:00:00')\n",
    "date_end = pd.to_datetime('2016-10-01 00:00:00')\n",
    "\n",
    "i = 1\n",
    "\n",
    "while True:\n",
    "    print(i)\n",
    "    df_reduced = df_dayton.loc[(df_dayton['ds'] >= date_start) & (df_dayton['ds'] < date_end)].copy()\n",
    "    model = model_class.RNN_FeatureAttention()\n",
    "    trainer = model_class.RNN_train_1(model, train_config)\n",
    "    forcast, true = trainer(df_reduced)\n",
    "    \n",
    "    result = pd.DataFrame({'day': df_reduced['ds'].dt.date.max(), 'hour': list(range(24)), 'y_hat': forcast, 'y': true})\n",
    "\n",
    "    results_aep_self_con = pd.concat([results_aep_self_con, result]).reset_index(drop=True)\n",
    "\n",
    "    date_start += pd.Timedelta(days=1)\n",
    "    date_end += pd.Timedelta(days=1)\n",
    "    if i == 70:\n",
    "        print('finish')\n",
    "        break\n",
    "    else:\n",
    "        i += 1\n",
    "\n",
    "results_aep_self_con.to_csv(out_dir + '/results_aep_self_con.csv', index=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "516ef30e-8db6-402f-8734-7ad8a9fb3b61",
   "metadata": {},
   "source": [
    "### test RNN+self convolution + numerical weekday"
   ]
  },
  {
   "cell_type": "code",
   "id": "8a3c678a-f2ff-4500-bd32-28f713423daf",
   "metadata": {},
   "source": [
    "train_config = model_class.training_config(n_epochs = 30, device = torch.device(\"mps\"))\n",
    "results_aep_self_con_wn = pd.DataFrame({'day': [], 'hour': [], 'y_hat': [], 'y':[]})\n",
    "date_start = pd.to_datetime('2012-10-01 00:00:00')\n",
    "date_end = pd.to_datetime('2016-10-01 00:00:00')\n",
    "\n",
    "i = 1\n",
    "\n",
    "while True:\n",
    "    print(i)\n",
    "    df_reduced = df_dayton.loc[(df_dayton['ds'] >= date_start) & (df_dayton['ds'] < date_end)].copy()\n",
    "    model = model_class.RNN_FeatureAttention_wn()\n",
    "    trainer = model_class.RNN_train_1(model, train_config)\n",
    "    forcast, true = trainer(df_reduced)\n",
    "    \n",
    "    result = pd.DataFrame({'day': df_reduced['ds'].dt.date.max(), 'hour': list(range(24)), 'y_hat': forcast, 'y': true})\n",
    "\n",
    "    results_aep_self_con_wn = pd.concat([results_aep_self_con_wn, result]).reset_index(drop=True)\n",
    "\n",
    "    date_start += pd.Timedelta(days=1)\n",
    "    date_end += pd.Timedelta(days=1)\n",
    "    if i == 70:\n",
    "        print('finish')\n",
    "        break\n",
    "    else:\n",
    "        i += 1\n",
    "\n",
    "results_aep_self_con_wn.to_csv(out_dir + '/results_aep_self_con_wn.csv', index=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0270a4ad-425a-48d9-8556-e4110c38d332",
   "metadata": {},
   "source": [
    "### test RNN+self convolution + categorical weekday"
   ]
  },
  {
   "cell_type": "code",
   "id": "7d71860d-6bd7-40b6-bce6-7e6f464f36d1",
   "metadata": {},
   "source": [
    "train_config = model_class.training_config(n_epochs = 30, device = torch.device(\"mps\"))\n",
    "results_aep_self_con_wc = pd.DataFrame({'day': [], 'hour': [], 'y_hat': [], 'y':[]})\n",
    "date_start = pd.to_datetime('2012-10-01 00:00:00')\n",
    "date_end = pd.to_datetime('2016-10-01 00:00:00')\n",
    "\n",
    "i = 1\n",
    "\n",
    "while True:\n",
    "    print(i)\n",
    "    df_reduced = df_dayton.loc[(df_dayton['ds'] >= date_start) & (df_dayton['ds'] < date_end)].copy()\n",
    "    model = model_class.RNN_FeatureAttention_wc()\n",
    "    trainer = model_class.RNN_train_2(model, train_config)\n",
    "    forcast, true = trainer(df_reduced)\n",
    "    \n",
    "    result = pd.DataFrame({'day': df_reduced['ds'].dt.date.max(), 'hour': list(range(24)), 'y_hat': forcast, 'y': true})\n",
    "\n",
    "    results_aep_self_con_wc = pd.concat([results_aep_self_con_wc, result]).reset_index(drop=True)\n",
    "\n",
    "    date_start += pd.Timedelta(days=1)\n",
    "    date_end += pd.Timedelta(days=1)\n",
    "    if i == 70:\n",
    "        print('finish')\n",
    "        break\n",
    "    else:\n",
    "        i += 1\n",
    "results_aep_self_con_wc.to_csv(out_dir + '/results_aep_self_con_wc.csv', index=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "486d9b49-266f-4d7a-adf2-50330ce2b43c",
   "metadata": {},
   "source": [
    "### test RNN+self convolution + fourier vector (only weekly)"
   ]
  },
  {
   "cell_type": "code",
   "id": "bf602c0e-24fa-4fdb-9d15-ea7053c447c5",
   "metadata": {},
   "source": [
    "train_config = model_class.training_config(n_epochs = 30, device = torch.device(\"mps\"))\n",
    "fourier_conf = model_class.fourier_config(mode=\"vector\", K_weekly=3, K_monthly=0)\n",
    "F_pairs = 2*(fourier_conf.K_weekly + fourier_conf.K_monthly)\n",
    "if fourier_conf.mode == \"vector\":\n",
    "    cont_dim = 1 + F_pairs\n",
    "    fourier_dim = F_pairs\n",
    "else:\n",
    "    cont_dim = 1 + 24*F_pairs\n",
    "    fourier_dim = F_pairs\n",
    "            \n",
    "results_aep_fourier_w = pd.DataFrame({'day': [], 'hour': [], 'y_hat': [], 'y':[]})\n",
    "date_start = pd.to_datetime('2012-10-01 00:00:00')\n",
    "date_end = pd.to_datetime('2016-10-01 00:00:00')\n",
    "\n",
    "i = 1\n",
    "\n",
    "while True:\n",
    "    print(i)\n",
    "    df_reduced = df_dayton.loc[(df_dayton['ds'] >= date_start) & (df_dayton['ds'] < date_end)].copy()\n",
    "    model = model_class.RNN_fourier(cont_dim=cont_dim, fourier_dim=fourier_dim,\n",
    "                                    xf_mode=fourier_conf.mode, d_model=128, nhead=4, \n",
    "                                    activation=\"relu\", learn_z0=True, dropout=0.0,\n",
    "                                    H=24, use_gate=True, nonneg_U0=False)\n",
    "    trainer = model_class.RNN_train_fourier(model, train_config, fourier_conf)\n",
    "    forcast, true = trainer(df_reduced)\n",
    "    \n",
    "    result = pd.DataFrame({'day': df_reduced['ds'].dt.date.max(), 'hour': list(range(24)), 'y_hat': forcast, 'y': true})\n",
    "\n",
    "    results_aep_fourier_w = pd.concat([results_aep_fourier_w, result]).reset_index(drop=True)\n",
    "\n",
    "    date_start += pd.Timedelta(days=1)\n",
    "    date_end += pd.Timedelta(days=1)\n",
    "    if i == 70:\n",
    "        print('finish')\n",
    "        break\n",
    "    else:\n",
    "        i += 1\n",
    "\n",
    "results_aep_fourier_w.to_csv(out_dir + '/results_aep_fourier_w.csv', index=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3833363e-8d81-4830-a91a-7669368eb946",
   "metadata": {},
   "source": [
    "### test RNN+self convolution + fourier matrix (only weekly)"
   ]
  },
  {
   "cell_type": "code",
   "id": "6bc4a3c9-4a45-4412-8ddb-702e2ef86549",
   "metadata": {},
   "source": [
    "train_config = model_class.training_config(n_epochs = 30, device = torch.device(\"mps\"))\n",
    "fourier_conf = model_class.fourier_config(mode=\"matrix\", K_weekly=3, K_monthly=0)\n",
    "F_pairs = 2*(fourier_conf.K_weekly + fourier_conf.K_monthly)\n",
    "if fourier_conf.mode == \"vector\":\n",
    "    cont_dim = 1 + F_pairs\n",
    "    fourier_dim = F_pairs\n",
    "else:\n",
    "    cont_dim = 1 + 24*F_pairs\n",
    "    fourier_dim = F_pairs\n",
    "            \n",
    "results_aep_fourier_w_matrix = pd.DataFrame({'day': [], 'hour': [], 'y_hat': [], 'y':[]})\n",
    "date_start = pd.to_datetime('2012-10-01 00:00:00')\n",
    "date_end = pd.to_datetime('2016-10-01 00:00:00')\n",
    "\n",
    "i = 1\n",
    "\n",
    "while True:\n",
    "    print(i)\n",
    "    df_reduced = df_dayton.loc[(df_dayton['ds'] >= date_start) & (df_dayton['ds'] < date_end)].copy()\n",
    "    model = model_class.RNN_fourier(cont_dim=cont_dim, fourier_dim=fourier_dim,\n",
    "                                    xf_mode=fourier_conf.mode, d_model=128, nhead=4, \n",
    "                                    activation=\"relu\", learn_z0=True, dropout=0.0,\n",
    "                                    H=24, use_gate=True, nonneg_U0=False)\n",
    "    trainer = model_class.RNN_train_fourier(model, train_config, fourier_conf)\n",
    "    forcast, true = trainer(df_reduced)\n",
    "    \n",
    "    result = pd.DataFrame({'day': df_reduced['ds'].dt.date.max(), 'hour': list(range(24)), 'y_hat': forcast, 'y': true})\n",
    "\n",
    "    results_aep_fourier_w_matrix = pd.concat([results_aep_fourier_w_matrix, result]).reset_index(drop=True)\n",
    "\n",
    "    date_start += pd.Timedelta(days=1)\n",
    "    date_end += pd.Timedelta(days=1)\n",
    "    if i == 70:\n",
    "        print('finish')\n",
    "        break\n",
    "    else:\n",
    "        i += 1\n",
    "\n",
    "results_aep_fourier_w_matrix.to_csv(out_dir + '/results_aep_fourier_w_matrix.csv', index=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "b0226858dbc63265",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_config = model_class.training_config(n_epochs = 30, device = torch.device(\"mps\"))\n",
    "fourier_conf = model_class.fourier_config(mode=\"matrix\", K_weekly=3, K_monthly=6, K_yearly=10)\n",
    "F_pairs = 2*(fourier_conf.K_weekly + fourier_conf.K_monthly + fourier_conf.K_yearly)\n",
    "if fourier_conf.mode == \"vector\":\n",
    "    cont_dim = 1 + F_pairs\n",
    "    fourier_dim = F_pairs\n",
    "else:\n",
    "    cont_dim = 1 + 24*F_pairs\n",
    "    fourier_dim = F_pairs\n",
    "\n",
    "results_aep_fourier_w_matrix = pd.DataFrame({'day': [], 'hour': [], 'y_hat': [], 'y':[]})\n",
    "date_start = pd.to_datetime('2012-10-01 00:00:00')\n",
    "date_end = pd.to_datetime('2016-10-01 00:00:00')\n",
    "\n",
    "i = 1\n",
    "\n",
    "while True:\n",
    "    print(i)\n",
    "    df_reduced = df_dayton.loc[(df_dayton['ds'] >= date_start) & (df_dayton['ds'] < date_end)].copy()\n",
    "    model = model_class.RNN_fourier(cont_dim=cont_dim, fourier_dim=fourier_dim,\n",
    "                                    xf_mode=fourier_conf.mode, d_model=128, nhead=4,\n",
    "                                    activation=\"relu\", learn_z0=True, dropout=0.0,\n",
    "                                    H=24, use_gate=True, nonneg_U0=False)\n",
    "    trainer = model_class.RNN_train_fourier(model, train_config, fourier_conf)\n",
    "    forcast, true = trainer(df_reduced)\n",
    "\n",
    "    result = pd.DataFrame({'day': df_reduced['ds'].dt.date.max(), 'hour': list(range(24)), 'y_hat': forcast, 'y': true})\n",
    "\n",
    "    results_aep_fourier_w_matrix = pd.concat([results_aep_fourier_w_matrix, result]).reset_index(drop=True)\n",
    "\n",
    "    date_start += pd.Timedelta(days=1)\n",
    "    date_end += pd.Timedelta(days=1)\n",
    "    if i == 70:\n",
    "        print('finish')\n",
    "        break\n",
    "    else:\n",
    "        i += 1\n",
    "\n",
    "results_aep_fourier_w_matrix.to_csv(out_dir + '/results_aep_fourier_kw3_km6_ky10_matrix.csv', index=False)"
   ],
   "id": "93729619fb65c27d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### test RNN_based asym",
   "id": "54d537e510ff3d03"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_config = model_class.training_config(n_epochs = 30, device = torch.device(\"mps\"))\n",
    "results_aep_asy_con = pd.DataFrame({'day': [], 'hour': [], 'y_hat': [], 'y':[]})\n",
    "date_start = pd.to_datetime('2012-10-01 00:00:00')\n",
    "date_end = pd.to_datetime('2016-10-01 00:00:00')\n",
    "\n",
    "i = 1\n",
    "\n",
    "while True:\n",
    "    print(i)\n",
    "    df_reduced = df_dayton.loc[(df_dayton['ds'] >= date_start) & (df_dayton['ds'] < date_end)].copy()\n",
    "    model = model_class.RNN_asym_feature()\n",
    "    trainer = model_class.RNN_train_1(model, train_config)\n",
    "    forcast, true = trainer(df_reduced)\n",
    "\n",
    "    result = pd.DataFrame({'day': df_reduced['ds'].dt.date.max(), 'hour': list(range(24)), 'y_hat': forcast, 'y': true})\n",
    "\n",
    "    results_aep_asy_con = pd.concat([results_aep_asy_con, result]).reset_index(drop=True)\n",
    "\n",
    "    date_start += pd.Timedelta(days=1)\n",
    "    date_end += pd.Timedelta(days=1)\n",
    "    if i == 70:\n",
    "        print('finish')\n",
    "        break\n",
    "    else:\n",
    "        i += 1\n",
    "\n",
    "results_aep_asy_con.to_csv(out_dir + '/results_aep_asy_con.csv', index=False)"
   ],
   "id": "11bc72e4cced646e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### test RNN_based ARMA",
   "id": "6a504d406fd2c7bb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_config = model_class.training_config(n_epochs = 30, device = torch.device(\"cpu\"))\n",
    "results_arma_asy_con = pd.DataFrame({'day': [], 'hour': [], 'y_hat': [], 'y':[]})\n",
    "date_start = pd.to_datetime('2012-10-01 00:00:00')\n",
    "date_end = pd.to_datetime('2016-10-01 00:00:00')\n",
    "\n",
    "i = 1\n",
    "\n",
    "while True:\n",
    "    print(i)\n",
    "    df_reduced = df_dayton.loc[(df_dayton['ds'] >= date_start) & (df_dayton['ds'] < date_end)].copy()\n",
    "    model = model_class.RNN_ARMA_Feature()\n",
    "    trainer = model_class.RNN_train_1(model, train_config)\n",
    "    forcast, true = trainer(df_reduced)\n",
    "    result = pd.DataFrame({'day': df_reduced['ds'].dt.date.max(), 'hour': list(range(24)), 'y_hat': forcast, 'y': true})\n",
    "\n",
    "    results_arma_asy_con = pd.concat([results_arma_asy_con, result]).reset_index(drop=True)\n",
    "\n",
    "    date_start += pd.Timedelta(days=1)\n",
    "    date_end += pd.Timedelta(days=1)\n",
    "    if i == 70:\n",
    "        print('finish')\n",
    "        break\n",
    "    else:\n",
    "        i += 1\n",
    "\n",
    "results_arma_asy_con.to_csv(out_dir + '/results_arma_asy_con.csv', index=False)"
   ],
   "id": "e3e66e64470977a0",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d27cf356-2ff4-4378-b703-2f448c5feb51",
   "metadata": {},
   "source": [
    "## DEOK"
   ]
  },
  {
   "cell_type": "code",
   "id": "6a3a7744-611e-43a6-af04-b599caa3f0f9",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from prophet import Prophet\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import model_summary as model_class\n",
    "import prophet_linear_adjust as prophet_based\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "58e77d59-04d2-4325-ba43-49365711bf25",
   "metadata": {},
   "source": [
    "df_deok = pd.read_csv(\"DEOK_hourly.csv\")\n",
    "df_deok.rename(columns={'Datetime': 'ds', 'DEOK_MW': 'y'}, inplace=True)\n",
    "df_deok['ds'] = pd.to_datetime(df_deok['ds'], format='%Y-%m-%d %H:%M:%S')\n",
    "out_dir = 'Deok_results'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### test prophet based without seasonality",
   "id": "9c511e80846b31ce"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "daily_demand_deok = df_deok.groupby(df_deok['ds'].dt.date)['y'].sum().reset_index()\n",
    "date_start = pd.to_datetime('2013-10-01 00:00:00')\n",
    "date_end = pd.to_datetime('2017-10-01 00:00:00')\n",
    "\n",
    "results_prophet = []\n",
    "i = 1\n",
    "\n",
    "while True:\n",
    "\n",
    "    print(f\"fit model {date_end + pd.Timedelta(days=1)}\")\n",
    "    result = prophet_based.forecast_next_day_hourly(df_deok, date_start, date_end, daily_demand_deok, manual = False, daily = True)\n",
    "    result['h_ahead'] = pd.to_datetime(result['ds']).dt.hour\n",
    "    results_prophet.append(result)\n",
    "\n",
    "    date_start += pd.Timedelta(days=1)\n",
    "    date_end += pd.Timedelta(days=1)\n",
    "\n",
    "    if i == 70:\n",
    "        break\n",
    "    else:\n",
    "        i += 1\n",
    "\n",
    "results_prophet = pd.concat(results_prophet, ignore_index=True)\n",
    "\n",
    "results_prophet.to_csv(out_dir + '/results_prophet.csv', index=False)"
   ],
   "id": "2851f131cd7e7d1a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### test prophet based with weekly seasonality",
   "id": "bc68866fe05682cc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "daily_demand_deok = df_deok.groupby(df_deok['ds'].dt.date)['y'].sum().reset_index()\n",
    "date_start = pd.to_datetime('2013-10-01 00:00:00')\n",
    "date_end = pd.to_datetime('2017-10-01 00:00:00')\n",
    "\n",
    "results_prophet_w = []\n",
    "i = 1\n",
    "\n",
    "while True:\n",
    "\n",
    "    print(f\"fit model {date_end + pd.Timedelta(days=1)}\")\n",
    "    result = prophet_based.forecast_next_day_hourly(df_deok, date_start, date_end, daily_demand_deok, manual = False, daily = True, weekly = True)\n",
    "    result['h_ahead'] = pd.to_datetime(result['ds']).dt.hour\n",
    "    results_prophet_w.append(result)\n",
    "\n",
    "    date_start += pd.Timedelta(days=1)\n",
    "    date_end += pd.Timedelta(days=1)\n",
    "\n",
    "    if i == 70:\n",
    "        break\n",
    "    else:\n",
    "        i += 1\n",
    "\n",
    "results_prophet_w = pd.concat(results_prophet_w, ignore_index=True)\n",
    "\n",
    "results_prophet_w.to_csv(out_dir + '/results_prophet_w.csv', index=False)"
   ],
   "id": "237529760dfba1c",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9c39d431-2b1f-4c1a-beb4-7419b9743eff",
   "metadata": {},
   "source": [
    "### test RNN+self convolution"
   ]
  },
  {
   "cell_type": "code",
   "id": "56191aed-b3f0-4310-844e-7a7cb33859c7",
   "metadata": {},
   "source": [
    "train_config = model_class.training_config(n_epochs = 30, device = torch.device(\"mps\"))\n",
    "results_aep_self_con = pd.DataFrame({'day': [], 'hour': [], 'y_hat': [], 'y':[]})\n",
    "date_start = pd.to_datetime('2013-10-01 00:00:00')\n",
    "date_end = pd.to_datetime('2017-10-01 00:00:00')\n",
    "\n",
    "i = 1\n",
    "\n",
    "while True:\n",
    "    print(i)\n",
    "    df_reduced = df_deok.loc[(df_deok['ds'] >= date_start) & (df_deok['ds'] < date_end)].copy()\n",
    "    model = model_class.RNN_FeatureAttention()\n",
    "    trainer = model_class.RNN_train_1(model, train_config)\n",
    "    forcast, true = trainer(df_reduced)\n",
    "    \n",
    "    result = pd.DataFrame({'day': df_reduced['ds'].dt.date.max(), 'hour': list(range(24)), 'y_hat': forcast, 'y': true})\n",
    "\n",
    "    results_aep_self_con = pd.concat([results_aep_self_con, result]).reset_index(drop=True)\n",
    "\n",
    "    date_start += pd.Timedelta(days=1)\n",
    "    date_end += pd.Timedelta(days=1)\n",
    "    if i == 70:\n",
    "        print('finish')\n",
    "        break\n",
    "    else:\n",
    "        i += 1\n",
    "\n",
    "results_aep_self_con.to_csv(out_dir + '/results_aep_self_con.csv', index=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3b0ad29d-c1ce-42e4-b531-c18995aee2a5",
   "metadata": {},
   "source": [
    "### test RNN+self convolution + numerical weekday"
   ]
  },
  {
   "cell_type": "code",
   "id": "d1a634a6-8595-4e14-9211-321c96e3952e",
   "metadata": {},
   "source": [
    "train_config = model_class.training_config(n_epochs = 30, device = torch.device(\"mps\"))\n",
    "results_aep_self_con_wn = pd.DataFrame({'day': [], 'hour': [], 'y_hat': [], 'y':[]})\n",
    "date_start = pd.to_datetime('2013-10-01 00:00:00')\n",
    "date_end = pd.to_datetime('2017-10-01 00:00:00')\n",
    "\n",
    "i = 1\n",
    "\n",
    "while True:\n",
    "    print(i)\n",
    "    df_reduced = df_deok.loc[(df_deok['ds'] >= date_start) & (df_deok['ds'] < date_end)].copy()\n",
    "    model = model_class.RNN_FeatureAttention_wn()\n",
    "    trainer = model_class.RNN_train_1(model, train_config)\n",
    "    forcast, true = trainer(df_reduced)\n",
    "    \n",
    "    result = pd.DataFrame({'day': df_reduced['ds'].dt.date.max(), 'hour': list(range(24)), 'y_hat': forcast, 'y': true})\n",
    "\n",
    "    results_aep_self_con_wn = pd.concat([results_aep_self_con_wn, result]).reset_index(drop=True)\n",
    "\n",
    "    date_start += pd.Timedelta(days=1)\n",
    "    date_end += pd.Timedelta(days=1)\n",
    "    if i == 70:\n",
    "        print('finish')\n",
    "        break\n",
    "    else:\n",
    "        i += 1\n",
    "\n",
    "results_aep_self_con_wn.to_csv(out_dir + '/results_aep_self_con_wn.csv', index=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d279f1e3-814f-4e92-a728-a69b8d9edaa7",
   "metadata": {},
   "source": [
    "### test RNN+self convolution + categorical weekday"
   ]
  },
  {
   "cell_type": "code",
   "id": "9f2c37ad-3a27-4288-9e33-e4fd6762084b",
   "metadata": {},
   "source": [
    "train_config = model_class.training_config(n_epochs = 30, device = torch.device(\"mps\"))\n",
    "results_aep_self_con_wc = pd.DataFrame({'day': [], 'hour': [], 'y_hat': [], 'y':[]})\n",
    "date_start = pd.to_datetime('2013-10-01 00:00:00')\n",
    "date_end = pd.to_datetime('2017-10-01 00:00:00')\n",
    "\n",
    "i = 1\n",
    "\n",
    "while True:\n",
    "    print(i)\n",
    "    df_reduced = df_deok.loc[(df_deok['ds'] >= date_start) & (df_deok['ds'] < date_end)].copy()\n",
    "    model = model_class.RNN_FeatureAttention_wc()\n",
    "    trainer = model_class.RNN_train_2(model, train_config)\n",
    "    forcast, true = trainer(df_reduced)\n",
    "    \n",
    "    result = pd.DataFrame({'day': df_reduced['ds'].dt.date.max(), 'hour': list(range(24)), 'y_hat': forcast, 'y': true})\n",
    "\n",
    "    results_aep_self_con_wc = pd.concat([results_aep_self_con_wc, result]).reset_index(drop=True)\n",
    "\n",
    "    date_start += pd.Timedelta(days=1)\n",
    "    date_end += pd.Timedelta(days=1)\n",
    "    if i == 70:\n",
    "        print('finish')\n",
    "        break\n",
    "    else:\n",
    "        i += 1\n",
    "results_aep_self_con_wc.to_csv(out_dir + '/results_aep_self_con_wc.csv', index=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0b3f52c4-720d-43c7-bdf7-6feff3156197",
   "metadata": {},
   "source": [
    "### test RNN+self convolution + fourier vector (only weekly)"
   ]
  },
  {
   "cell_type": "code",
   "id": "4b06fe62-65ac-4279-8eaf-af83e1993db9",
   "metadata": {},
   "source": [
    "train_config = model_class.training_config(n_epochs = 30, device = torch.device(\"mps\"))\n",
    "fourier_conf = model_class.fourier_config(mode=\"vector\", K_weekly=3, K_monthly=0)\n",
    "F_pairs = 2*(fourier_conf.K_weekly + fourier_conf.K_monthly)\n",
    "if fourier_conf.mode == \"vector\":\n",
    "    cont_dim = 1 + F_pairs\n",
    "    fourier_dim = F_pairs\n",
    "else:\n",
    "    cont_dim = 1 + 24*F_pairs\n",
    "    fourier_dim = F_pairs\n",
    "            \n",
    "results_aep_fourier_w = pd.DataFrame({'day': [], 'hour': [], 'y_hat': [], 'y':[]})\n",
    "date_start = pd.to_datetime('2013-10-01 00:00:00')\n",
    "date_end = pd.to_datetime('2017-10-01 00:00:00')\n",
    "\n",
    "i = 1\n",
    "\n",
    "while True:\n",
    "    print(i)\n",
    "    df_reduced = df_deok.loc[(df_deok['ds'] >= date_start) & (df_deok['ds'] < date_end)].copy()\n",
    "    model = model_class.RNN_fourier(cont_dim=cont_dim, fourier_dim=fourier_dim,\n",
    "                                    xf_mode=fourier_conf.mode, d_model=128, nhead=4, \n",
    "                                    activation=\"relu\", learn_z0=True, dropout=0.0,\n",
    "                                    H=24, use_gate=True, nonneg_U0=False)\n",
    "    trainer = model_class.RNN_train_fourier(model, train_config, fourier_conf)\n",
    "    forcast, true = trainer(df_reduced)\n",
    "    \n",
    "    result = pd.DataFrame({'day': df_reduced['ds'].dt.date.max(), 'hour': list(range(24)), 'y_hat': forcast, 'y': true})\n",
    "\n",
    "    results_aep_fourier_w = pd.concat([results_aep_fourier_w, result]).reset_index(drop=True)\n",
    "\n",
    "    date_start += pd.Timedelta(days=1)\n",
    "    date_end += pd.Timedelta(days=1)\n",
    "    if i == 70:\n",
    "        print('finish')\n",
    "        break\n",
    "    else:\n",
    "        i += 1\n",
    "\n",
    "results_aep_fourier_w.to_csv(out_dir + '/results_aep_fourier_w.csv', index=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9a8eb99c-9fd8-4e1f-bde8-d828c435296c",
   "metadata": {},
   "source": [
    "### test RNN+self convolution + fourier matrix (only weekly)"
   ]
  },
  {
   "cell_type": "code",
   "id": "b17cb594-359d-4712-b704-71b7e293481c",
   "metadata": {},
   "source": [
    "train_config = model_class.training_config(n_epochs = 30, device = torch.device(\"mps\"))\n",
    "fourier_conf = model_class.fourier_config(mode=\"matrix\", K_weekly=3, K_monthly=0)\n",
    "F_pairs = 2*(fourier_conf.K_weekly + fourier_conf.K_monthly)\n",
    "if fourier_conf.mode == \"vector\":\n",
    "    cont_dim = 1 + F_pairs\n",
    "    fourier_dim = F_pairs\n",
    "else:\n",
    "    cont_dim = 1 + 24*F_pairs\n",
    "    fourier_dim = F_pairs\n",
    "            \n",
    "results_aep_fourier_w_matrix = pd.DataFrame({'day': [], 'hour': [], 'y_hat': [], 'y':[]})\n",
    "date_start = pd.to_datetime('2013-10-01 00:00:00')\n",
    "date_end = pd.to_datetime('2017-10-01 00:00:00')\n",
    "\n",
    "i = 1\n",
    "\n",
    "while True:\n",
    "    print(i)\n",
    "    df_reduced = df_deok.loc[(df_deok['ds'] >= date_start) & (df_deok['ds'] < date_end)].copy()\n",
    "    model = model_class.RNN_fourier(cont_dim=cont_dim, fourier_dim=fourier_dim,\n",
    "                                    xf_mode=fourier_conf.mode, d_model=128, nhead=4, \n",
    "                                    activation=\"relu\", learn_z0=True, dropout=0.0,\n",
    "                                    H=24, use_gate=True, nonneg_U0=False)\n",
    "    trainer = model_class.RNN_train_fourier(model, train_config, fourier_conf)\n",
    "    forcast, true = trainer(df_reduced)\n",
    "    \n",
    "    result = pd.DataFrame({'day': df_reduced['ds'].dt.date.max(), 'hour': list(range(24)), 'y_hat': forcast, 'y': true})\n",
    "\n",
    "    results_aep_fourier_w_matrix = pd.concat([results_aep_fourier_w_matrix, result]).reset_index(drop=True)\n",
    "\n",
    "    date_start += pd.Timedelta(days=1)\n",
    "    date_end += pd.Timedelta(days=1)\n",
    "    if i == 70:\n",
    "        print('finish')\n",
    "        break\n",
    "    else:\n",
    "        i += 1\n",
    "\n",
    "results_aep_fourier_w_matrix.to_csv(out_dir + '/results_aep_fourier_w_matrix.csv', index=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### test RNN_based asym",
   "id": "961ea56b8e152fde"
  },
  {
   "cell_type": "code",
   "id": "80b8d677-64cd-46f4-beec-91da417b3940",
   "metadata": {},
   "source": [
    "train_config = model_class.training_config(n_epochs = 30, device = torch.device(\"mps\"))\n",
    "results_aep_asy_con = pd.DataFrame({'day': [], 'hour': [], 'y_hat': [], 'y':[]})\n",
    "date_start = pd.to_datetime('2013-10-01 00:00:00')\n",
    "date_end = pd.to_datetime('2017-10-01 00:00:00')\n",
    "\n",
    "i = 1\n",
    "\n",
    "while True:\n",
    "    print(i)\n",
    "    df_reduced = df_deok.loc[(df_deok['ds'] >= date_start) & (df_deok['ds'] < date_end)].copy()\n",
    "    model = model_class.RNN_asym_feature()\n",
    "    trainer = model_class.RNN_train_1(model, train_config)\n",
    "    forcast, true = trainer(df_reduced)\n",
    "\n",
    "    result = pd.DataFrame({'day': df_reduced['ds'].dt.date.max(), 'hour': list(range(24)), 'y_hat': forcast, 'y': true})\n",
    "\n",
    "    results_aep_asy_con = pd.concat([results_aep_asy_con, result]).reset_index(drop=True)\n",
    "\n",
    "    date_start += pd.Timedelta(days=1)\n",
    "    date_end += pd.Timedelta(days=1)\n",
    "    if i == 70:\n",
    "        print('finish')\n",
    "        break\n",
    "    else:\n",
    "        i += 1\n",
    "\n",
    "results_aep_asy_con.to_csv(out_dir + '/results_aep_asy_con.csv', index=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Test RNN_based ARMA",
   "id": "dbbcaa33b4e564a6"
  },
  {
   "cell_type": "code",
   "id": "c7c23a4c-b0d3-487f-9550-c71b8b5d9d9a",
   "metadata": {},
   "source": [
    "train_config = model_class.training_config(n_epochs = 30, device = torch.device(\"cpu\"))\n",
    "results_arma_asy_con = pd.DataFrame({'day': [], 'hour': [], 'y_hat': [], 'y':[]})\n",
    "date_start = pd.to_datetime('2013-10-01 00:00:00')\n",
    "date_end = pd.to_datetime('2017-10-01 00:00:00')\n",
    "\n",
    "i = 1\n",
    "\n",
    "while True:\n",
    "    print(i)\n",
    "    df_reduced = df_deok.loc[(df_deok['ds'] >= date_start) & (df_deok['ds'] < date_end)].copy()\n",
    "    model = model_class.RNN_ARMA_Feature()\n",
    "    trainer = model_class.RNN_train_1(model, train_config)\n",
    "    forcast, true = trainer(df_reduced)\n",
    "    result = pd.DataFrame({'day': df_reduced['ds'].dt.date.max(), 'hour': list(range(24)), 'y_hat': forcast, 'y': true})\n",
    "\n",
    "    results_arma_asy_con = pd.concat([results_arma_asy_con, result]).reset_index(drop=True)\n",
    "\n",
    "    date_start += pd.Timedelta(days=1)\n",
    "    date_end += pd.Timedelta(days=1)\n",
    "    if i == 70:\n",
    "        print('finish')\n",
    "        break\n",
    "    else:\n",
    "        i += 1\n",
    "\n",
    "results_arma_asy_con.to_csv(out_dir + '/results_arma_asy_con.csv', index=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "aa01b47f-ab24-4859-8e8c-c1697ed2e61a",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1664e3b4-3076-40f6-9971-fde328252947",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "446fb538-8d01-444d-b722-99e2547467c9",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f269cf35-b68b-4c0f-81dc-eb4cf3b80359",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "41807dc5-6529-47ea-857e-483dacaca4a3",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "75b79ce5-fbc3-4ea9-9c8d-804118a32c6f",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "52b3181d-41c4-4e29-a7e4-622ffe9eaa62",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6266e9ec-31c0-434f-87d9-74fa9156e69e",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ea2f8595-bad6-4254-97d7-c473ef63525a",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4418937a-a255-464f-8efb-7c01372490d6",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-11T10:49:38.931370Z",
     "start_time": "2025-11-11T10:49:38.926161Z"
    }
   },
   "source": "",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### AEP",
   "id": "f0ed47a98b689f55"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T10:27:41.879976Z",
     "start_time": "2025-11-11T10:27:40.902498Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from prophet import Prophet\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import prophet_linear_adjust as prophet_based"
   ],
   "id": "d75cb3348af6f810",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Importing plotly failed. Interactive plots will not work.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def simple_dst_fix(df: pd.DataFrame, start_at_midnight: bool = True) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[\"ds\"] = pd.to_datetime(df[\"ds\"])\n",
    "    df = df.sort_values(\"ds\")\n",
    "\n",
    "    df = df[~df[\"ds\"].duplicated(keep=\"first\")]\n",
    "\n",
    "    start = df[\"ds\"].iloc[0]\n",
    "    if start_at_midnight:\n",
    "        start = start.normalize()\n",
    "    end = df[\"ds\"].iloc[-1]\n",
    "    full_idx = pd.date_range(start, end, freq=\"h\")\n",
    "\n",
    "    out = df.set_index(\"ds\").reindex(full_idx)\n",
    "\n",
    "    num_cols = out.select_dtypes(include=\"number\").columns\n",
    "    out[num_cols] = out[num_cols].ffill()\n",
    "\n",
    "    if out[num_cols].isna().any().any():\n",
    "        out[num_cols] = out[num_cols].bfill()\n",
    "\n",
    "    out = out.rename_axis(\"ds\").reset_index()\n",
    "\n",
    "    return out"
   ],
   "id": "a85e954ccd065256"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T10:27:45.009761Z",
     "start_time": "2025-11-11T10:27:44.925116Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_aep = pd.read_csv(\"data/AEP_hourly.csv\")\n",
    "df_aep.rename(columns={'Datetime': 'ds', 'AEP_MW': 'y'}, inplace=True)\n",
    "df_aep['ds'] = pd.to_datetime(df_aep['ds'], format='%Y-%m-%d %H:%M:%S')\n",
    "df_aep = simple_dst_fix(df_aep)\n",
    "out_dir = 'AEP_results'"
   ],
   "id": "814dabafb443e93e",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### test propht model",
   "id": "49522d4da00c9081"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T09:07:20.759605Z",
     "start_time": "2025-11-11T08:52:15.297253Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import prophet_linear_adjust_yearly_to_hourly\n",
    "\n",
    "yearly_demand = df_aep.groupby(df_aep['ds'].dt.year)['y'].sum().reset_index()\n",
    "date_start = pd.to_datetime('2005-01-01 00:00:00')\n",
    "date_end = pd.to_datetime('2015-01-01 00:00:00')\n",
    "\n",
    "results_prophet_yearly = []\n",
    "i = 1\n",
    "\n",
    "while True:\n",
    "    print(i)\n",
    "    result = prophet_linear_adjust_yearly_to_hourly.forecast_next_year_hourly(\n",
    "        df_aep, date_start, date_end, yearly_demand,\n",
    "        bayesian_samples=0,\n",
    "        manual=False,\n",
    "        daily=True,\n",
    "        weekly = True,\n",
    "        yearly = True,\n",
    "        monthly = True)\n",
    "\n",
    "    results_prophet_yearly.append(result)\n",
    "\n",
    "    date_start += pd.DateOffset(years=1)\n",
    "    date_end += pd.DateOffset(years=1)\n",
    "\n",
    "    if i == 3:\n",
    "        break\n",
    "    else:\n",
    "        i += 1\n",
    "\n",
    "results_prophet_yearly = pd.concat(results_prophet_yearly, ignore_index=True)\n",
    "results_prophet_yearly.to_csv('experiment_results/' + out_dir + '/results_prophet_yearly.csv', index=False)\n"
   ],
   "id": "e8208ba57254cfbd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02:52:18 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:52:37 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02:57:48 - cmdstanpy - INFO - Chain [1] start processing\n",
      "02:58:11 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03:02:56 - cmdstanpy - INFO - Chain [1] start processing\n",
      "Python(48722) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "03:03:16 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### test RNN augmented Prophet model",
   "id": "2d362d6a9452ba1e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T10:43:20.386881Z",
     "start_time": "2025-11-11T10:28:33.830461Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import prophet_yearly_to_daily\n",
    "import RNN_fourier_RNN_uncertainty\n",
    "\n",
    "train_config = RNN_fourier_RNN_uncertainty.training_config(n_epochs=30, device=torch.device(\"mps\"))\n",
    "fourier_conf = RNN_fourier_RNN_uncertainty.fourier_config(mode=\"matrix\", K_weekly=3, K_monthly=6, K_yearly=10)\n",
    "\n",
    "K_total = fourier_conf.K_weekly + fourier_conf.K_monthly + fourier_conf.K_yearly\n",
    "F_per_hour = 2 * K_total\n",
    "\n",
    "if fourier_conf.mode == \"vector\":\n",
    "    cont_dim = 1 + F_per_hour\n",
    "    fourier_dim = F_per_hour\n",
    "else:\n",
    "    cont_dim = 1 + 24 * F_per_hour\n",
    "    fourier_dim = F_per_hour\n",
    "\n",
    "df_daily = df_aep.copy()\n",
    "df_daily['ds'] = pd.to_datetime(df_daily['ds'])\n",
    "df_daily['date'] = df_daily['ds'].dt.date\n",
    "df_daily_agg = df_daily.groupby('date')['y'].sum().reset_index()\n",
    "df_daily_agg.columns = ['ds', 'y']\n",
    "df_daily_agg['ds'] = pd.to_datetime(df_daily_agg['ds'])\n",
    "\n",
    "yearly_demand = df_daily_agg.copy()\n",
    "yearly_demand['year'] = yearly_demand['ds'].dt.year\n",
    "yearly_demand = yearly_demand.groupby('year')['y'].sum().reset_index()\n",
    "yearly_demand.columns = ['ds', 'y']\n",
    "\n",
    "results_hybrid = pd.DataFrame({'ds': [], 'y_hat': [], 'y': []})\n",
    "\n",
    "date_start = pd.to_datetime('2005-01-01 00:00:00')\n",
    "date_end = pd.to_datetime('2015-01-01 00:00:00')\n",
    "\n",
    "i = 1\n",
    "\n",
    "while True:\n",
    "\n",
    "    print(i)\n",
    "    df_train_day = df_daily_agg.loc[(df_daily_agg['ds'] >= date_start) & (df_daily_agg['ds'] < date_end)].copy()\n",
    "    df_test_day = df_daily_agg.loc[(df_daily_agg['ds'] >= date_end) & (df_daily_agg['ds'] < date_end + pd.DateOffset(years=1))].copy()\n",
    "\n",
    "    # ===== STAGE 1: Prophet predicts daily loads for the next year =====\n",
    "    result_prophet = prophet_yearly_to_daily.forecast_next_year_daily(\n",
    "        df_train_day,\n",
    "        df_test_day,\n",
    "        yearly_demand,\n",
    "        manual=False,\n",
    "        weekly=True,\n",
    "        monthly=True,\n",
    "        yearly=True\n",
    "    )\n",
    "    result_prophet = result_prophet.sort_values('ds').reset_index()\n",
    "\n",
    "    df_train_hour = df_aep.loc[(df_aep['ds'] >= date_start) & (df_aep['ds'] < date_end)].copy().sort_values('ds').reset_index(drop=True)\n",
    "    df_test_hour = df_aep.loc[(df_aep['ds'] >= date_end) & (df_aep['ds'] < date_end + pd.DateOffset(years=1))].copy().sort_values('ds').reset_index(drop=True)\n",
    "\n",
    "    model = RNN_fourier_RNN_uncertainty.RNN_fourier(\n",
    "        cont_dim=cont_dim,\n",
    "        fourier_dim=fourier_dim,\n",
    "        xf_mode=\"matrix\",\n",
    "        d_model = 128,\n",
    "        latent_dim=32,\n",
    "        nhead=4\n",
    "    )\n",
    "\n",
    "    trainer = RNN_fourier_RNN_uncertainty.RNN_train_fourier(model, train_config, fourier_conf)\n",
    "    trainer(df_train_hour)\n",
    "\n",
    "    daily_prediction = result_prophet[['ds','adjusted_y']]\n",
    "    daily_prediction.columns = ['ds', 'y']\n",
    "\n",
    "    fake_hourly_load = []\n",
    "    for _, row in daily_prediction.iterrows():\n",
    "        day = row['ds']\n",
    "        daily_val = row['y'] / 24\n",
    "        # create 24 hourly entries for this day\n",
    "        for h in range(24):\n",
    "            fake_hourly_load.append({\n",
    "                'ds': pd.Timestamp(day) + pd.Timedelta(hours=h),\n",
    "                'y': daily_val\n",
    "            })\n",
    "\n",
    "    # convert to DataFrame\n",
    "    fake_hourly_load = pd.DataFrame(fake_hourly_load)\n",
    "\n",
    "    forcast, _ = trainer.forecate(fake_hourly_load, deterministic = True)\n",
    "\n",
    "    result = pd.DataFrame({'ds': df_test_hour['ds'], 'y_hat': forcast, 'y': df_test_hour['y']})\n",
    "\n",
    "    results_hybrid = pd.concat([results_hybrid, result]).reset_index(drop=True)\n",
    "\n",
    "    results_hybrid.to_csv('experiment_results/' + out_dir + '/results_prophet_rnn_hybrid.csv', index=False)\n",
    "\n",
    "    date_start += pd.DateOffset(years=1)\n",
    "    date_end += pd.DateOffset(years=1)\n",
    "\n",
    "    if i  == 3:\n",
    "        break\n",
    "\n",
    "    else:\n",
    "        i = i + 1\n",
    "\n",
    "results_hybrid.to_csv('experiment_results/' + out_dir + '/results_prophet_rnn_hybrid.csv', index=False)"
   ],
   "id": "7ae2ba66d3565fa6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04:28:34 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04:28:34 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 loss: 0.1635\n",
      "epoch 2 loss: 0.0772\n",
      "epoch 3 loss: 0.0571\n",
      "epoch 4 loss: 0.0459\n",
      "epoch 5 loss: 0.0440\n",
      "epoch 6 loss: 0.0522\n",
      "epoch 7 loss: 0.0415\n",
      "epoch 8 loss: 0.0373\n",
      "epoch 9 loss: 0.0342\n",
      "epoch 10 loss: 0.0408\n",
      "epoch 11 loss: 0.0394\n",
      "epoch 12 loss: 0.0374\n",
      "epoch 13 loss: 0.0379\n",
      "epoch 14 loss: 0.0370\n",
      "epoch 15 loss: 0.0352\n",
      "epoch 16 loss: 0.0380\n",
      "epoch 17 loss: 0.0317\n",
      "epoch 18 loss: 0.0353\n",
      "epoch 19 loss: 0.0356\n",
      "epoch 20 loss: 0.0366\n",
      "epoch 21 loss: 0.0402\n",
      "epoch 22 loss: 0.0348\n",
      "epoch 23 loss: 0.0403\n",
      "epoch 24 loss: 0.0330\n",
      "epoch 25 loss: 0.0376\n",
      "epoch 26 loss: 0.0367\n",
      "epoch 27 loss: 0.0346\n",
      "epoch 28 loss: 0.0301\n",
      "epoch 29 loss: 0.0361\n",
      "epoch 30 loss: 0.0335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3h/5yfj7yqx7lg98t310bfxxr3m0000gn/T/ipykernel_50767/691296404.py:93: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results_hybrid = pd.concat([results_hybrid, result]).reset_index(drop=True)\n",
      "04:33:10 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04:33:10 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 loss: 0.1827\n",
      "epoch 2 loss: 0.0743\n",
      "epoch 3 loss: 0.0538\n",
      "epoch 4 loss: 0.0458\n",
      "epoch 5 loss: 0.0448\n",
      "epoch 6 loss: 0.0371\n",
      "epoch 7 loss: 0.0407\n",
      "epoch 8 loss: 0.0376\n",
      "epoch 9 loss: 0.0365\n",
      "epoch 10 loss: 0.0384\n",
      "epoch 11 loss: 0.0386\n",
      "epoch 12 loss: 0.0368\n",
      "epoch 13 loss: 0.0381\n",
      "epoch 14 loss: 0.0370\n",
      "epoch 15 loss: 0.0392\n",
      "epoch 16 loss: 0.0398\n",
      "epoch 17 loss: 0.0333\n",
      "epoch 18 loss: 0.0359\n",
      "epoch 19 loss: 0.0325\n",
      "epoch 20 loss: 0.0325\n",
      "epoch 21 loss: 0.0347\n",
      "epoch 22 loss: 0.0309\n",
      "epoch 23 loss: 0.0353\n",
      "epoch 24 loss: 0.0300\n",
      "epoch 25 loss: 0.0342\n",
      "epoch 26 loss: 0.0307\n",
      "epoch 27 loss: 0.0305\n",
      "epoch 28 loss: 0.0325\n",
      "epoch 29 loss: 0.0299\n",
      "epoch 30 loss: 0.0307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04:38:41 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04:38:41 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 loss: 0.0875\n",
      "epoch 2 loss: 0.0515\n",
      "epoch 3 loss: 0.0425\n",
      "epoch 4 loss: 0.0414\n",
      "epoch 5 loss: 0.0371\n",
      "epoch 6 loss: 0.0384\n",
      "epoch 7 loss: 0.0341\n",
      "epoch 8 loss: 0.0385\n",
      "epoch 9 loss: 0.0368\n",
      "epoch 10 loss: 0.0385\n",
      "epoch 11 loss: 0.0384\n",
      "epoch 12 loss: 0.0397\n",
      "epoch 13 loss: 0.0297\n",
      "epoch 14 loss: 0.0406\n",
      "epoch 15 loss: 0.0348\n",
      "epoch 16 loss: 0.0334\n",
      "epoch 17 loss: 0.0337\n",
      "epoch 18 loss: 0.0302\n",
      "epoch 19 loss: 0.0336\n",
      "epoch 20 loss: 0.0306\n",
      "epoch 21 loss: 0.0340\n",
      "epoch 22 loss: 0.0349\n",
      "epoch 23 loss: 0.0328\n",
      "epoch 24 loss: 0.0328\n",
      "epoch 25 loss: 0.0334\n",
      "epoch 26 loss: 0.0316\n",
      "epoch 27 loss: 0.0316\n",
      "epoch 28 loss: 0.0296\n",
      "epoch 29 loss: 0.0337\n",
      "epoch 30 loss: 0.0323\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### test 2 stage RNN",
   "id": "c9947b5ce46709a2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T10:19:19.295979Z",
     "start_time": "2025-11-11T10:19:19.290693Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import Hierarchical_RNN_2stage_uncertainty as hrnn\n",
    "import os\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Configuration\n",
    "# ============================================================================\n",
    "\n",
    "fourier_config = hrnn.FourierConfig(\n",
    "    K_yearly_to_daily=8,\n",
    "    K_monthly_to_daily=6,\n",
    "    K_weekly_to_daily=8,\n",
    "    K_yearly_to_hourly=10,\n",
    "    K_monthly_to_hourly=7,\n",
    "    K_daily_to_hourly=8,\n",
    "    K_weekly_to_hourly=3,\n",
    ")\n",
    "\n",
    "train_config = hrnn.TrainingConfig(\n",
    "    base_epochs=200,\n",
    "    yearly_to_daily_multiplier=6,\n",
    "    daily_to_hourly_multiplier=1,\n",
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    lr=5e-4,\n",
    "    lambda0=1e-6,\n",
    "    lambdaf=1e-5,\n",
    "    batch_size=32,\n",
    "    T_seq_yearly=3,\n",
    "    T_seq_daily=32,\n",
    "    curriculum_start_prob=1.0,\n",
    "    curriculum_end_prob=1.0\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# Results Storage\n",
    "# ============================================================================\n",
    "\n",
    "results_hierarchical = pd.DataFrame({\n",
    "    'year': [],\n",
    "    'month': [],\n",
    "    'day': [],\n",
    "    'hour': [],\n",
    "    'y_hat': [],\n",
    "    'y': []\n",
    "})\n",
    "\n",
    "# ============================================================================\n",
    "# Rolling Window Setup\n",
    "# ============================================================================\n",
    "\n",
    "date_start = pd.to_datetime('2008-01-01 00:00:00')\n",
    "date_end = pd.to_datetime('2015-01-01 00:00:00')\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"2-Stage Hierarchical RNN - Deterministic Prediction\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Initial training window: {date_start.date()} to {date_end.date()}\")\n",
    "print(f\"Window size: ~10 years\")\n",
    "print(f\"Prediction: 1 year ahead (deterministic)\")\n",
    "print(f\"T_seq_yearly: {train_config.T_seq_yearly}\")\n",
    "print(f\"T_seq_daily: {train_config.T_seq_daily}\")\n",
    "print(f\"Device: {train_config.device}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(f'experiment_results/{out_dir}', exist_ok=True)\n",
    "\n",
    "# ============================================================================\n",
    "# Rolling Window Testing Loop\n",
    "# ============================================================================\n",
    "\n",
    "i = 1\n",
    "\n",
    "while True:\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"Iteration {i}\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Get training data\n",
    "    df_train = df_aep.loc[(df_aep['ds'] >= date_start) & (df_aep['ds'] < date_end)].copy().sort_values(by='ds').reset_index(drop=True)\n",
    "\n",
    "    # Get test year\n",
    "    test_year_start = date_end\n",
    "    test_year_end = date_end + pd.DateOffset(years=1)\n",
    "    df_test = df_aep.loc[(df_aep['ds'] >= test_year_start) & (df_aep['ds'] < test_year_end)].copy().sort_values(by='ds').reset_index(drop=True)\n",
    "\n",
    "    if len(df_test) == 0:\n",
    "        print(\"No more test data available. Stopping.\")\n",
    "        break\n",
    "\n",
    "    print(f\"Training: {date_start.date()} to {date_end.date()}\")\n",
    "    print(f\"Testing:  {test_year_start.date()} to {test_year_end.date()}\")\n",
    "    print(f\"Training samples: {len(df_train)}\")\n",
    "    print(f\"Test samples: {len(df_test)}\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # Build and Train Model\n",
    "    # ========================================================================\n",
    "\n",
    "    print(\"\\nBuilding model...\")\n",
    "    model_config = dict(\n",
    "        fourier_config=fourier_config,\n",
    "        training_config=train_config,\n",
    "        latent_dim=36,\n",
    "        d_model=128,\n",
    "        nhead=4\n",
    "    )\n",
    "    downscaler = hrnn.build_model_dp(hrnn.HierarchicalDownscaler_2Stage, **model_config)\n",
    "\n",
    "    # Train model using Approach 2 (Curriculum Learning)\n",
    "    print(\"\\nTraining model...\")\n",
    "    trainer = hrnn.Approach2_CurriculumLearning_2Stage(downscaler)\n",
    "    trainer.train(df_train)\n",
    "\n",
    "    # ========================================================================\n",
    "    # Generate Deterministic Prediction\n",
    "    # ========================================================================\n",
    "\n",
    "    test_year_int = test_year_start.year\n",
    "    yearly_sum = df_test['y'].sum()\n",
    "\n",
    "    print(f\"\\nTest year: {test_year_int}\")\n",
    "    print(f\"Actual yearly sum: {yearly_sum:.2f}\")\n",
    "\n",
    "    # Prepare historical data from training\n",
    "    data_dict_train = downscaler.module.prepare_data(df_train) if hasattr(downscaler, 'module') else downscaler.prepare_data(df_train)\n",
    "\n",
    "    # Get yearly history for Stage 1\n",
    "    df_yearly_train = data_dict_train['yearly'].sort_values('year')\n",
    "    historical_years = [{'year': row['year'], 'yearly_sum': row['yearly_sum']} for _, row in df_yearly_train.iterrows()]\n",
    "\n",
    "    print(f\"Using {len(historical_years)} historical years as context\")\n",
    "\n",
    "    print(f\"\\nGenerating deterministic prediction for year {test_year_int}...\")\n",
    "    predictor = hrnn.HierarchicalPredictor_2Stage(downscaler)\n",
    "\n",
    "    y_hat = predictor.predict_yearly_to_hourly(\n",
    "        yearly_sum=yearly_sum,\n",
    "        year=test_year_int,\n",
    "        historical_years=historical_years\n",
    "    )\n",
    "\n",
    "    print(f\"Generated predictions of shape {y_hat.shape}\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # Align with Test Data\n",
    "    # ========================================================================\n",
    "\n",
    "    n_hours_actual = len(df_test)\n",
    "    n_hours_pred = len(y_hat)\n",
    "\n",
    "    if n_hours_pred > n_hours_actual:\n",
    "        y_hat = y_hat[:n_hours_actual]\n",
    "        print(f\"Trimmed predictions from {n_hours_pred} to {n_hours_actual} hours\")\n",
    "    elif n_hours_pred < n_hours_actual:\n",
    "        # Pad if needed\n",
    "        padding_value = y_hat.mean()\n",
    "        pad_width = n_hours_actual - n_hours_pred\n",
    "        y_hat = np.pad(y_hat, (0, pad_width), 'constant', constant_values=padding_value)\n",
    "        print(f\"Padded predictions from {n_hours_pred} to {n_hours_actual} hours\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # Store Results\n",
    "    # ========================================================================\n",
    "\n",
    "    y_true = df_test['y'].values\n",
    "\n",
    "    result = pd.DataFrame({\n",
    "        'year': df_test['ds'].dt.year,\n",
    "        'month': df_test['ds'].dt.month,\n",
    "        'day': df_test['ds'].dt.day,\n",
    "        'hour': df_test['ds'].dt.hour,\n",
    "        'y_hat': y_hat,\n",
    "        'y': y_true\n",
    "    })\n",
    "\n",
    "    # Append to results\n",
    "    results_hierarchical = pd.concat(\n",
    "        [results_hierarchical, result],\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "    # ========================================================================\n",
    "    # Save Intermediate Results\n",
    "    # ========================================================================\n",
    "\n",
    "    if i % 1 == 0:\n",
    "        print(f\"\\nSaving intermediate results...\")\n",
    "        results_hierarchical.to_csv(\n",
    "            f'experiment_results/{out_dir}/results_hierarchical.csv',\n",
    "            index=False\n",
    "        )\n",
    "\n",
    "    # ========================================================================\n",
    "    # Move Window Forward\n",
    "    # ========================================================================\n",
    "\n",
    "    date_start += pd.DateOffset(years=1)\n",
    "    date_end += pd.DateOffset(years=1)\n",
    "\n",
    "    # Stop after 3 years of testing\n",
    "    if i == 3:\n",
    "        print('\\n' + '=' * 70)\n",
    "        print('Finished 3 iterations!')\n",
    "        print('=' * 70)\n",
    "        break\n",
    "    else:\n",
    "        i += 1\n",
    "\n",
    "# ============================================================================\n",
    "# Final Save\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Saving final results...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "results_hierarchical.to_csv(\n",
    "    f'experiment_results/{out_dir}/results_hierarchical.csv',\n",
    "    index=False\n",
    ")\n",
    "\n",
    "print(\"\\nAll results saved successfully!\")\n",
    "print(f\"Output directory: experiment_results/{out_dir}/\")\n",
    "print(\"\\nFiles created:\")\n",
    "print(\"  - results_hierarchical.csv\")"
   ],
   "id": "54839bbe0aa5666d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8760\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Comed",
   "id": "4add5b40c9a24d8b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T11:36:40.833686Z",
     "start_time": "2025-11-11T11:36:40.827697Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from prophet import Prophet\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error"
   ],
   "id": "d654a1cf9be55d2e",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T11:36:42.141985Z",
     "start_time": "2025-11-11T11:36:42.137662Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def simple_dst_fix(df: pd.DataFrame, start_at_midnight: bool = True) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[\"ds\"] = pd.to_datetime(df[\"ds\"])\n",
    "    df = df.sort_values(\"ds\")\n",
    "\n",
    "    df = df[~df[\"ds\"].duplicated(keep=\"first\")]\n",
    "\n",
    "    start = df[\"ds\"].iloc[0]\n",
    "    if start_at_midnight:\n",
    "        start = start.normalize()\n",
    "    end = df[\"ds\"].iloc[-1]\n",
    "    full_idx = pd.date_range(start, end, freq=\"h\")\n",
    "\n",
    "    out = df.set_index(\"ds\").reindex(full_idx)\n",
    "\n",
    "    num_cols = out.select_dtypes(include=\"number\").columns\n",
    "    out[num_cols] = out[num_cols].ffill()\n",
    "\n",
    "    if out[num_cols].isna().any().any():\n",
    "        out[num_cols] = out[num_cols].bfill()\n",
    "\n",
    "    out = out.rename_axis(\"ds\").reset_index()\n",
    "\n",
    "    return out"
   ],
   "id": "16254a02029c5d77",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T11:36:43.590335Z",
     "start_time": "2025-11-11T11:36:43.513113Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_comed = pd.read_csv(\"data/COMED_hourly.csv\")\n",
    "df_comed.rename(columns={'Datetime': 'ds', 'COMED_MW': 'y'}, inplace=True)\n",
    "df_comed['ds'] = pd.to_datetime(df_comed['ds'], format='%Y-%m-%d %H:%M:%S')\n",
    "df_comed = simple_dst_fix(df_comed)\n",
    "out_dir = 'Comed_results'"
   ],
   "id": "f40e720efc90d505",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### test prophet",
   "id": "14551c724e52c98c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T11:10:22.479270Z",
     "start_time": "2025-11-11T10:50:03.521807Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import prophet_linear_adjust_yearly_to_hourly\n",
    "\n",
    "yearly_demand = df_comed.groupby(df_comed['ds'].dt.year)['y'].sum().reset_index()\n",
    "date_start = pd.to_datetime('2011-01-01 00:00:00')\n",
    "date_end = pd.to_datetime('2015-01-01 00:00:00')\n",
    "\n",
    "results_prophet_yearly = []\n",
    "i = 1\n",
    "\n",
    "while True:\n",
    "    print(i)\n",
    "    result = prophet_linear_adjust_yearly_to_hourly.forecast_next_year_hourly(\n",
    "        df_comed, date_start, date_end, yearly_demand,\n",
    "        bayesian_samples=0,\n",
    "        manual=False,\n",
    "        daily=True,\n",
    "        weekly = True,\n",
    "        yearly = True,\n",
    "        monthly = True)\n",
    "\n",
    "    results_prophet_yearly.append(result)\n",
    "\n",
    "    date_start += pd.DateOffset(years=1)\n",
    "    date_end += pd.DateOffset(years=1)\n",
    "\n",
    "    if i == 3:\n",
    "        break\n",
    "    else:\n",
    "        i += 1\n",
    "\n",
    "results_prophet_yearly = pd.concat(results_prophet_yearly, ignore_index=True)\n",
    "results_prophet_yearly.to_csv('experiment_results/' + out_dir + '/results_prophet_yearly.csv', index=False)\n"
   ],
   "id": "3a7752f7edc2e374",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04:50:04 - cmdstanpy - INFO - Chain [1] start processing\n",
      "04:50:17 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04:57:47 - cmdstanpy - INFO - Chain [1] start processing\n",
      "04:57:59 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05:03:40 - cmdstanpy - INFO - Chain [1] start processing\n",
      "05:03:49 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### test RNN augmented Prophet model",
   "id": "8140cde5335f599e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T11:42:16.369086Z",
     "start_time": "2025-11-11T11:36:57.548234Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import prophet_yearly_to_daily\n",
    "import RNN_fourier_RNN_uncertainty\n",
    "\n",
    "train_config = RNN_fourier_RNN_uncertainty.training_config(n_epochs=30, device=torch.device(\"mps\"))\n",
    "fourier_conf = RNN_fourier_RNN_uncertainty.fourier_config(mode=\"matrix\", K_weekly=3, K_monthly=6, K_yearly=10)\n",
    "\n",
    "K_total = fourier_conf.K_weekly + fourier_conf.K_monthly + fourier_conf.K_yearly\n",
    "F_per_hour = 2 * K_total\n",
    "\n",
    "if fourier_conf.mode == \"vector\":\n",
    "    cont_dim = 1 + F_per_hour\n",
    "    fourier_dim = F_per_hour\n",
    "else:\n",
    "    cont_dim = 1 + 24 * F_per_hour\n",
    "    fourier_dim = F_per_hour\n",
    "\n",
    "df_daily = df_comed.copy()\n",
    "df_daily['ds'] = pd.to_datetime(df_daily['ds'])\n",
    "df_daily['date'] = df_daily['ds'].dt.date\n",
    "df_daily_agg = df_daily.groupby('date')['y'].sum().reset_index()\n",
    "df_daily_agg.columns = ['ds', 'y']\n",
    "df_daily_agg['ds'] = pd.to_datetime(df_daily_agg['ds'])\n",
    "\n",
    "yearly_demand = df_daily_agg.copy()\n",
    "yearly_demand['year'] = yearly_demand['ds'].dt.year\n",
    "yearly_demand = yearly_demand.groupby('year')['y'].sum().reset_index()\n",
    "yearly_demand.columns = ['ds', 'y']\n",
    "\n",
    "results_hybrid = pd.DataFrame({'ds': [], 'y_hat': [], 'y': []})\n",
    "\n",
    "date_start = pd.to_datetime('2011-01-01 00:00:00')\n",
    "date_end = pd.to_datetime('2015-01-01 00:00:00')\n",
    "\n",
    "i = 1\n",
    "\n",
    "while True:\n",
    "\n",
    "    print(i)\n",
    "    df_train_day = df_daily_agg.loc[(df_daily_agg['ds'] >= date_start) & (df_daily_agg['ds'] < date_end)].copy()\n",
    "    df_test_day = df_daily_agg.loc[(df_daily_agg['ds'] >= date_end) & (df_daily_agg['ds'] < date_end + pd.DateOffset(years=1))].copy()\n",
    "\n",
    "    # ===== STAGE 1: Prophet predicts daily loads for the next year =====\n",
    "    result_prophet = prophet_yearly_to_daily.forecast_next_year_daily(\n",
    "        df_train_day,\n",
    "        df_test_day,\n",
    "        yearly_demand,\n",
    "        manual=False,\n",
    "        weekly=True,\n",
    "        monthly=True,\n",
    "        yearly=True\n",
    "    )\n",
    "    result_prophet = result_prophet.sort_values('ds').reset_index()\n",
    "\n",
    "    df_train_hour = df_comed.loc[(df_comed['ds'] >= date_start) & (df_comed['ds'] < date_end)].copy().sort_values('ds').reset_index(drop=True)\n",
    "    df_test_hour = df_comed.loc[(df_comed['ds'] >= date_end) & (df_comed['ds'] < date_end + pd.DateOffset(years=1))].copy().sort_values('ds').reset_index(drop=True)\n",
    "\n",
    "    model = RNN_fourier_RNN_uncertainty.RNN_fourier(\n",
    "        cont_dim=cont_dim,\n",
    "        fourier_dim=fourier_dim,\n",
    "        xf_mode=\"matrix\",\n",
    "        d_model = 128,\n",
    "        latent_dim=32,\n",
    "        nhead=4\n",
    "    )\n",
    "\n",
    "    trainer = RNN_fourier_RNN_uncertainty.RNN_train_fourier(model, train_config, fourier_conf)\n",
    "    trainer(df_train_hour)\n",
    "\n",
    "    daily_prediction = result_prophet[['ds','adjusted_y']]\n",
    "    daily_prediction.columns = ['ds', 'y']\n",
    "\n",
    "    fake_hourly_load = []\n",
    "    for _, row in daily_prediction.iterrows():\n",
    "        day = row['ds']\n",
    "        daily_val = row['y'] / 24\n",
    "        for h in range(24):\n",
    "            fake_hourly_load.append({\n",
    "                'ds': pd.Timestamp(day) + pd.Timedelta(hours=h),\n",
    "                'y': daily_val\n",
    "            })\n",
    "\n",
    "    # convert to DataFrame\n",
    "    fake_hourly_load = pd.DataFrame(fake_hourly_load)\n",
    "\n",
    "    forcast, _ = trainer.forecate(fake_hourly_load, deterministic = True)\n",
    "\n",
    "    result = pd.DataFrame({'ds': df_test_hour['ds'], 'y_hat': forcast, 'y': df_test_hour['y']})\n",
    "\n",
    "    results_hybrid = pd.concat([results_hybrid, result]).reset_index(drop=True)\n",
    "\n",
    "    date_start += pd.DateOffset(years=1)\n",
    "    date_end += pd.DateOffset(years=1)\n",
    "\n",
    "    if i  == 3:\n",
    "        break\n",
    "\n",
    "    else:\n",
    "        i = i + 1\n",
    "\n",
    "results_hybrid.to_csv('experiment_results/' + out_dir + '/results_prophet_rnn_hybrid.csv', index=False)"
   ],
   "id": "363cb2a485156aab",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05:36:57 - cmdstanpy - INFO - Chain [1] start processing\n",
      "05:36:57 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zero/Documents/GitHub/RNN_based_downscaling/RNN_fourier_RNN_uncertainty.py:495: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.\n",
      "Consider using tensor.detach() first. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/python_variable_methods.cpp:837.)\n",
      "  print(f\"epoch {ep + 1} loss: {float(loss):.4f}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 loss: 0.4941\n",
      "epoch 2 loss: 0.1456\n",
      "epoch 3 loss: 0.1051\n",
      "epoch 4 loss: 0.0648\n",
      "epoch 5 loss: 0.0699\n",
      "epoch 6 loss: 0.0694\n",
      "epoch 7 loss: 0.0422\n",
      "epoch 8 loss: 0.0384\n",
      "epoch 9 loss: 0.0421\n",
      "epoch 10 loss: 0.0435\n",
      "epoch 11 loss: 0.0268\n",
      "epoch 12 loss: 0.0322\n",
      "epoch 13 loss: 0.0402\n",
      "epoch 14 loss: 0.0314\n",
      "epoch 15 loss: 0.0372\n",
      "epoch 16 loss: 0.0402\n",
      "epoch 17 loss: 0.0272\n",
      "epoch 18 loss: 0.0277\n",
      "epoch 19 loss: 0.0301\n",
      "epoch 20 loss: 0.0246\n",
      "epoch 21 loss: 0.0279\n",
      "epoch 22 loss: 0.0338\n",
      "epoch 23 loss: 0.0359\n",
      "epoch 24 loss: 0.0255\n",
      "epoch 25 loss: 0.0258\n",
      "epoch 26 loss: 0.0256\n",
      "epoch 27 loss: 0.0271\n",
      "epoch 28 loss: 0.0264\n",
      "epoch 29 loss: 0.0262\n",
      "epoch 30 loss: 0.0351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3h/5yfj7yqx7lg98t310bfxxr3m0000gn/T/ipykernel_53280/403997022.py:92: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results_hybrid = pd.concat([results_hybrid, result]).reset_index(drop=True)\n",
      "05:38:47 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05:38:48 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 loss: 0.3052\n",
      "epoch 2 loss: 0.2069\n",
      "epoch 3 loss: 0.1433\n",
      "epoch 4 loss: 0.1355\n",
      "epoch 5 loss: 0.1252\n",
      "epoch 6 loss: 0.0787\n",
      "epoch 7 loss: 0.0654\n",
      "epoch 8 loss: 0.1013\n",
      "epoch 9 loss: 0.0487\n",
      "epoch 10 loss: 0.0655\n",
      "epoch 11 loss: 0.0513\n",
      "epoch 12 loss: 0.0512\n",
      "epoch 13 loss: 0.0763\n",
      "epoch 14 loss: 0.0469\n",
      "epoch 15 loss: 0.0441\n",
      "epoch 16 loss: 0.0516\n",
      "epoch 17 loss: 0.0371\n",
      "epoch 18 loss: 0.0331\n",
      "epoch 19 loss: 0.0502\n",
      "epoch 20 loss: 0.0383\n",
      "epoch 21 loss: 0.0370\n",
      "epoch 22 loss: 0.0388\n",
      "epoch 23 loss: 0.0315\n",
      "epoch 24 loss: 0.0339\n",
      "epoch 25 loss: 0.0403\n",
      "epoch 26 loss: 0.0351\n",
      "epoch 27 loss: 0.0295\n",
      "epoch 28 loss: 0.0279\n",
      "epoch 29 loss: 0.0318\n",
      "epoch 30 loss: 0.0313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05:40:33 - cmdstanpy - INFO - Chain [1] start processing\n",
      "05:40:33 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "epoch 1 loss: 0.4401\n",
      "epoch 2 loss: 0.2247\n",
      "epoch 3 loss: 0.1539\n",
      "epoch 4 loss: 0.1120\n",
      "epoch 5 loss: 0.0855\n",
      "epoch 6 loss: 0.0741\n",
      "epoch 7 loss: 0.0603\n",
      "epoch 8 loss: 0.0539\n",
      "epoch 9 loss: 0.0489\n",
      "epoch 10 loss: 0.0479\n",
      "epoch 11 loss: 0.0403\n",
      "epoch 12 loss: 0.0328\n",
      "epoch 13 loss: 0.0379\n",
      "epoch 14 loss: 0.0343\n",
      "epoch 15 loss: 0.0302\n",
      "epoch 16 loss: 0.0296\n",
      "epoch 17 loss: 0.0343\n",
      "epoch 18 loss: 0.0339\n",
      "epoch 19 loss: 0.0327\n",
      "epoch 20 loss: 0.0393\n",
      "epoch 21 loss: 0.0310\n",
      "epoch 22 loss: 0.0357\n",
      "epoch 23 loss: 0.0336\n",
      "epoch 24 loss: 0.0319\n",
      "epoch 25 loss: 0.0289\n",
      "epoch 26 loss: 0.0289\n",
      "epoch 27 loss: 0.0314\n",
      "epoch 28 loss: 0.0285\n",
      "epoch 29 loss: 0.0319\n",
      "epoch 30 loss: 0.0314\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### test 2 stage RNN",
   "id": "f4b73a5c3a3e781f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import Hierarchical_RNN_2stage_uncertainty as hrnn\n",
    "import os\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Configuration\n",
    "# ============================================================================\n",
    "\n",
    "fourier_config = hrnn.FourierConfig(\n",
    "    K_yearly_to_daily=8,\n",
    "    K_monthly_to_daily=6,\n",
    "    K_weekly_to_daily=8,\n",
    "    K_yearly_to_hourly=10,\n",
    "    K_monthly_to_hourly=7,\n",
    "    K_daily_to_hourly=8,\n",
    "    K_weekly_to_hourly=3,\n",
    ")\n",
    "\n",
    "train_config = hrnn.TrainingConfig(\n",
    "    base_epochs=200,\n",
    "    yearly_to_daily_multiplier=6,\n",
    "    daily_to_hourly_multiplier=1,\n",
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    lr=5e-4,\n",
    "    lambda0=1e-6,\n",
    "    lambdaf=1e-5,\n",
    "    batch_size=32,\n",
    "    T_seq_yearly=3,\n",
    "    T_seq_daily=32,\n",
    "    curriculum_start_prob=1.0,\n",
    "    curriculum_end_prob=1.0\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# Results Storage\n",
    "# ============================================================================\n",
    "\n",
    "results_hierarchical = pd.DataFrame({\n",
    "    'year': [],\n",
    "    'month': [],\n",
    "    'day': [],\n",
    "    'hour': [],\n",
    "    'y_hat': [],\n",
    "    'y': []\n",
    "})\n",
    "\n",
    "# ============================================================================\n",
    "# Rolling Window Setup\n",
    "# ============================================================================\n",
    "\n",
    "date_start = pd.to_datetime('2011-01-01 00:00:00')\n",
    "date_end = pd.to_datetime('2015-01-01 00:00:00')\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"2-Stage Hierarchical RNN - Deterministic Prediction\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Initial training window: {date_start.date()} to {date_end.date()}\")\n",
    "print(f\"Window size: ~10 years\")\n",
    "print(f\"Prediction: 1 year ahead (deterministic)\")\n",
    "print(f\"T_seq_yearly: {train_config.T_seq_yearly}\")\n",
    "print(f\"T_seq_daily: {train_config.T_seq_daily}\")\n",
    "print(f\"Device: {train_config.device}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(f'experiment_results/{out_dir}', exist_ok=True)\n",
    "\n",
    "# ============================================================================\n",
    "# Rolling Window Testing Loop\n",
    "# ============================================================================\n",
    "\n",
    "i = 1\n",
    "\n",
    "while True:\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"Iteration {i}\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Get training data\n",
    "    df_train = df_comed.loc[(df_comed['ds'] >= date_start) & (df_comed['ds'] < date_end)].copy().sort_values(by='ds').reset_index(drop=True)\n",
    "\n",
    "    # Get test year\n",
    "    test_year_start = date_end\n",
    "    test_year_end = date_end + pd.DateOffset(years=1)\n",
    "    df_test = df_comed.loc[(df_comed['ds'] >= test_year_start) & (df_comed['ds'] < test_year_end)].copy().sort_values(by='ds').reset_index(drop=True)\n",
    "\n",
    "    if len(df_test) == 0:\n",
    "        print(\"No more test data available. Stopping.\")\n",
    "        break\n",
    "\n",
    "    print(f\"Training: {date_start.date()} to {date_end.date()}\")\n",
    "    print(f\"Testing:  {test_year_start.date()} to {test_year_end.date()}\")\n",
    "    print(f\"Training samples: {len(df_train)}\")\n",
    "    print(f\"Test samples: {len(df_test)}\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # Build and Train Model\n",
    "    # ========================================================================\n",
    "\n",
    "    print(\"\\nBuilding model...\")\n",
    "    model_config = dict(\n",
    "        fourier_config=fourier_config,\n",
    "        training_config=train_config,\n",
    "        latent_dim=36,\n",
    "        d_model=128,\n",
    "        nhead=4\n",
    "    )\n",
    "    downscaler = hrnn.build_model_dp(hrnn.HierarchicalDownscaler_2Stage, **model_config)\n",
    "\n",
    "    # Train model using Approach 2 (Curriculum Learning)\n",
    "    print(\"\\nTraining model...\")\n",
    "    trainer = hrnn.Approach2_CurriculumLearning_2Stage(downscaler)\n",
    "    trainer.train(df_train)\n",
    "\n",
    "    # ========================================================================\n",
    "    # Generate Deterministic Prediction\n",
    "    # ========================================================================\n",
    "\n",
    "    test_year_int = test_year_start.year\n",
    "    yearly_sum = df_test['y'].sum()\n",
    "\n",
    "    print(f\"\\nTest year: {test_year_int}\")\n",
    "    print(f\"Actual yearly sum: {yearly_sum:.2f}\")\n",
    "\n",
    "    # Prepare historical data from training\n",
    "    data_dict_train = downscaler.module.prepare_data(df_train) if hasattr(downscaler, 'module') else downscaler.prepare_data(df_train)\n",
    "\n",
    "    # Get yearly history for Stage 1\n",
    "    df_yearly_train = data_dict_train['yearly'].sort_values('year')\n",
    "    historical_years = [{'year': row['year'], 'yearly_sum': row['yearly_sum']} for _, row in df_yearly_train.iterrows()]\n",
    "\n",
    "    print(f\"Using {len(historical_years)} historical years as context\")\n",
    "\n",
    "    print(f\"\\nGenerating deterministic prediction for year {test_year_int}...\")\n",
    "    predictor = hrnn.HierarchicalPredictor_2Stage(downscaler)\n",
    "\n",
    "    y_hat = predictor.predict_yearly_to_hourly(\n",
    "        yearly_sum=yearly_sum,\n",
    "        year=test_year_int,\n",
    "        historical_years=historical_years\n",
    "    )\n",
    "\n",
    "    print(f\"Generated predictions of shape {y_hat.shape}\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # Align with Test Data\n",
    "    # ========================================================================\n",
    "\n",
    "    n_hours_actual = len(df_test)\n",
    "    n_hours_pred = len(y_hat)\n",
    "\n",
    "    if n_hours_pred > n_hours_actual:\n",
    "        y_hat = y_hat[:n_hours_actual]\n",
    "        print(f\"Trimmed predictions from {n_hours_pred} to {n_hours_actual} hours\")\n",
    "    elif n_hours_pred < n_hours_actual:\n",
    "        # Pad if needed\n",
    "        padding_value = y_hat.mean()\n",
    "        pad_width = n_hours_actual - n_hours_pred\n",
    "        y_hat = np.pad(y_hat, (0, pad_width), 'constant', constant_values=padding_value)\n",
    "        print(f\"Padded predictions from {n_hours_pred} to {n_hours_actual} hours\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # Store Results\n",
    "    # ========================================================================\n",
    "\n",
    "    y_true = df_test['y'].values\n",
    "\n",
    "    result = pd.DataFrame({\n",
    "        'year': df_test['ds'].dt.year,\n",
    "        'month': df_test['ds'].dt.month,\n",
    "        'day': df_test['ds'].dt.day,\n",
    "        'hour': df_test['ds'].dt.hour,\n",
    "        'y_hat': y_hat,\n",
    "        'y': y_true\n",
    "    })\n",
    "\n",
    "    # Append to results\n",
    "    results_hierarchical = pd.concat(\n",
    "        [results_hierarchical, result],\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "    # ========================================================================\n",
    "    # Save Intermediate Results\n",
    "    # ========================================================================\n",
    "\n",
    "    if i % 1 == 0:\n",
    "        print(f\"\\nSaving intermediate results...\")\n",
    "        results_hierarchical.to_csv(\n",
    "            f'experiment_results/{out_dir}/results_hierarchical.csv',\n",
    "            index=False\n",
    "        )\n",
    "\n",
    "    # ========================================================================\n",
    "    # Move Window Forward\n",
    "    # ========================================================================\n",
    "\n",
    "    date_start += pd.DateOffset(years=1)\n",
    "    date_end += pd.DateOffset(years=1)\n",
    "\n",
    "    # Stop after 3 years of testing\n",
    "    if i == 3:\n",
    "        print('\\n' + '=' * 70)\n",
    "        print('Finished 3 iterations!')\n",
    "        print('=' * 70)\n",
    "        break\n",
    "    else:\n",
    "        i += 1\n",
    "\n",
    "# ============================================================================\n",
    "# Final Save\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Saving final results...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "results_hierarchical.to_csv(\n",
    "    f'experiment_results/{out_dir}/results_hierarchical.csv',\n",
    "    index=False\n",
    ")\n",
    "\n",
    "print(\"\\nAll results saved successfully!\")\n",
    "print(f\"Output directory: experiment_results/{out_dir}/\")\n",
    "print(\"\\nFiles created:\")\n",
    "print(\"  - results_hierarchical.csv\")"
   ],
   "id": "2b1a3c5975b460ee"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Dayton",
   "id": "dd14bf08edc51aa8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T12:04:49.172022Z",
     "start_time": "2025-11-11T12:04:48.169750Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from prophet import Prophet\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n"
   ],
   "id": "5e02ad669af2c4d4",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Importing plotly failed. Interactive plots will not work.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T12:04:50.235103Z",
     "start_time": "2025-11-11T12:04:50.231224Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def simple_dst_fix(df: pd.DataFrame, start_at_midnight: bool = True) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[\"ds\"] = pd.to_datetime(df[\"ds\"])\n",
    "    df = df.sort_values(\"ds\")\n",
    "\n",
    "    df = df[~df[\"ds\"].duplicated(keep=\"first\")]\n",
    "\n",
    "    start = df[\"ds\"].iloc[0]\n",
    "    if start_at_midnight:\n",
    "        start = start.normalize()\n",
    "    end = df[\"ds\"].iloc[-1]\n",
    "    full_idx = pd.date_range(start, end, freq=\"h\")\n",
    "\n",
    "    out = df.set_index(\"ds\").reindex(full_idx)\n",
    "\n",
    "    num_cols = out.select_dtypes(include=\"number\").columns\n",
    "    out[num_cols] = out[num_cols].ffill()\n",
    "\n",
    "    if out[num_cols].isna().any().any():\n",
    "        out[num_cols] = out[num_cols].bfill()\n",
    "\n",
    "    out = out.rename_axis(\"ds\").reset_index()\n",
    "\n",
    "    return out"
   ],
   "id": "a45d97e9af688023",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T12:04:51.952777Z",
     "start_time": "2025-11-11T12:04:51.868844Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_dayton = pd.read_csv(\"data/DAYTON_hourly.csv\")\n",
    "df_dayton.rename(columns={'Datetime': 'ds', 'DAYTON_MW': 'y'}, inplace=True)\n",
    "df_dayton['ds'] = pd.to_datetime(df_dayton['ds'], format='%Y-%m-%d %H:%M:%S')\n",
    "df_dayton = simple_dst_fix(df_dayton)\n",
    "out_dir = 'Dayton_results'"
   ],
   "id": "d94c9b8d8f5ea946",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T12:23:48.919746Z",
     "start_time": "2025-11-11T12:04:56.923119Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import prophet_linear_adjust_yearly_to_hourly\n",
    "\n",
    "yearly_demand = df_dayton.groupby(df_dayton['ds'].dt.year)['y'].sum().reset_index()\n",
    "date_start = pd.to_datetime('2005-01-01 00:00:00')\n",
    "date_end = pd.to_datetime('2015-01-01 00:00:00')\n",
    "\n",
    "results_prophet_yearly = []\n",
    "i = 1\n",
    "\n",
    "while True:\n",
    "    print(i)\n",
    "    result = prophet_linear_adjust_yearly_to_hourly.forecast_next_year_hourly(\n",
    "        df_dayton, date_start, date_end, yearly_demand,\n",
    "        bayesian_samples=0,\n",
    "        manual=False,\n",
    "        daily=True,\n",
    "        weekly = True,\n",
    "        yearly = True,\n",
    "        monthly = True)\n",
    "\n",
    "    results_prophet_yearly.append(result)\n",
    "\n",
    "    date_start += pd.DateOffset(years=1)\n",
    "    date_end += pd.DateOffset(years=1)\n",
    "\n",
    "    if i == 3:\n",
    "        break\n",
    "    else:\n",
    "        i += 1\n",
    "\n",
    "results_prophet_yearly = pd.concat(results_prophet_yearly, ignore_index=True)\n",
    "results_prophet_yearly.to_csv('experiment_results/' + out_dir + '/results_prophet_yearly.csv', index=False)\n"
   ],
   "id": "59b758d91f243b40",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06:05:00 - cmdstanpy - INFO - Chain [1] start processing\n",
      "06:05:19 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06:10:58 - cmdstanpy - INFO - Chain [1] start processing\n",
      "06:11:22 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06:17:42 - cmdstanpy - INFO - Chain [1] start processing\n",
      "06:18:06 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "a8de87baaa9689dc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T12:39:25.903595Z",
     "start_time": "2025-11-11T12:26:08.716156Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import prophet_yearly_to_daily\n",
    "import RNN_fourier_RNN_uncertainty\n",
    "\n",
    "train_config = RNN_fourier_RNN_uncertainty.training_config(n_epochs=30, device=torch.device(\"mps\"))\n",
    "fourier_conf = RNN_fourier_RNN_uncertainty.fourier_config(mode=\"matrix\", K_weekly=3, K_monthly=6, K_yearly=10)\n",
    "\n",
    "K_total = fourier_conf.K_weekly + fourier_conf.K_monthly + fourier_conf.K_yearly\n",
    "F_per_hour = 2 * K_total\n",
    "\n",
    "if fourier_conf.mode == \"vector\":\n",
    "    cont_dim = 1 + F_per_hour\n",
    "    fourier_dim = F_per_hour\n",
    "else:\n",
    "    cont_dim = 1 + 24 * F_per_hour\n",
    "    fourier_dim = F_per_hour\n",
    "\n",
    "df_daily = df_dayton.copy()\n",
    "df_daily['ds'] = pd.to_datetime(df_daily['ds'])\n",
    "df_daily['date'] = df_daily['ds'].dt.date\n",
    "df_daily_agg = df_daily.groupby('date')['y'].sum().reset_index()\n",
    "df_daily_agg.columns = ['ds', 'y']\n",
    "df_daily_agg['ds'] = pd.to_datetime(df_daily_agg['ds'])\n",
    "\n",
    "yearly_demand = df_daily_agg.copy()\n",
    "yearly_demand['year'] = yearly_demand['ds'].dt.year\n",
    "yearly_demand = yearly_demand.groupby('year')['y'].sum().reset_index()\n",
    "yearly_demand.columns = ['ds', 'y']\n",
    "\n",
    "results_hybrid = pd.DataFrame({'ds': [], 'y_hat': [], 'y': []})\n",
    "\n",
    "date_start = pd.to_datetime('2005-01-01 00:00:00')\n",
    "date_end = pd.to_datetime('2015-01-01 00:00:00')\n",
    "\n",
    "i = 1\n",
    "\n",
    "while True:\n",
    "\n",
    "    print(i)\n",
    "    df_train_day = df_daily_agg.loc[(df_daily_agg['ds'] >= date_start) & (df_daily_agg['ds'] < date_end)].copy()\n",
    "    df_test_day = df_daily_agg.loc[(df_daily_agg['ds'] >= date_end) & (df_daily_agg['ds'] < date_end + pd.DateOffset(years=1))].copy()\n",
    "\n",
    "    # ===== STAGE 1: Prophet predicts daily loads for the next year =====\n",
    "    result_prophet = prophet_yearly_to_daily.forecast_next_year_daily(\n",
    "        df_train_day,\n",
    "        df_test_day,\n",
    "        yearly_demand,\n",
    "        manual=False,\n",
    "        weekly=True,\n",
    "        monthly=True,\n",
    "        yearly=True\n",
    "    )\n",
    "    result_prophet = result_prophet.sort_values('ds').reset_index()\n",
    "\n",
    "    df_train_hour = df_dayton.loc[(df_dayton['ds'] >= date_start) & (df_dayton['ds'] < date_end)].copy().sort_values('ds').reset_index(drop=True)\n",
    "    df_test_hour = df_dayton.loc[(df_dayton['ds'] >= date_end) & (df_dayton['ds'] < date_end + pd.DateOffset(years=1))].copy().sort_values('ds').reset_index(drop=True)\n",
    "\n",
    "    model = RNN_fourier_RNN_uncertainty.RNN_fourier(\n",
    "        cont_dim=cont_dim,\n",
    "        fourier_dim=fourier_dim,\n",
    "        xf_mode=\"matrix\",\n",
    "        d_model = 128,\n",
    "        latent_dim=32,\n",
    "        nhead=4\n",
    "    )\n",
    "\n",
    "    trainer = RNN_fourier_RNN_uncertainty.RNN_train_fourier(model, train_config, fourier_conf)\n",
    "    trainer(df_train_hour)\n",
    "\n",
    "    daily_prediction = result_prophet[['ds','adjusted_y']]\n",
    "    daily_prediction.columns = ['ds', 'y']\n",
    "\n",
    "    fake_hourly_load = []\n",
    "    for _, row in daily_prediction.iterrows():\n",
    "        day = row['ds']\n",
    "        daily_val = row['y'] / 24\n",
    "        for h in range(24):\n",
    "            fake_hourly_load.append({\n",
    "                'ds': pd.Timestamp(day) + pd.Timedelta(hours=h),\n",
    "                'y': daily_val\n",
    "            })\n",
    "\n",
    "    # convert to DataFrame\n",
    "    fake_hourly_load = pd.DataFrame(fake_hourly_load)\n",
    "\n",
    "    forcast, _ = trainer.forecate(fake_hourly_load, deterministic = True)\n",
    "\n",
    "    result = pd.DataFrame({'ds': df_test_hour['ds'], 'y_hat': forcast, 'y': df_test_hour['y']})\n",
    "\n",
    "    results_hybrid = pd.concat([results_hybrid, result]).reset_index(drop=True)\n",
    "\n",
    "    date_start += pd.DateOffset(years=1)\n",
    "    date_end += pd.DateOffset(years=1)\n",
    "    if i  == 3:\n",
    "        break\n",
    "\n",
    "    else:\n",
    "        i = i + 1\n",
    "\n",
    "results_hybrid.to_csv('experiment_results/' + out_dir + '/results_prophet_rnn_hybrid.csv', index=False)"
   ],
   "id": "ce7c7394698ebacc",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06:26:08 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(55170) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "06:26:09 - cmdstanpy - INFO - Chain [1] done processing\n",
      "/Users/zero/Documents/GitHub/RNN_based_downscaling/RNN_fourier_RNN_uncertainty.py:495: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.\n",
      "Consider using tensor.detach() first. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/python_variable_methods.cpp:837.)\n",
      "  print(f\"epoch {ep + 1} loss: {float(loss):.4f}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 loss: 0.1826\n",
      "epoch 2 loss: 0.1148\n",
      "epoch 3 loss: 0.0691\n",
      "epoch 4 loss: 0.0510\n",
      "epoch 5 loss: 0.0452\n",
      "epoch 6 loss: 0.0447\n",
      "epoch 7 loss: 0.0401\n",
      "epoch 8 loss: 0.0392\n",
      "epoch 9 loss: 0.0341\n",
      "epoch 10 loss: 0.0426\n",
      "epoch 11 loss: 0.0393\n",
      "epoch 12 loss: 0.0362\n",
      "epoch 13 loss: 0.0378\n",
      "epoch 14 loss: 0.0339\n",
      "epoch 15 loss: 0.0363\n",
      "epoch 16 loss: 0.0385\n",
      "epoch 17 loss: 0.0362\n",
      "epoch 18 loss: 0.0372\n",
      "epoch 19 loss: 0.0388\n",
      "epoch 20 loss: 0.0366\n",
      "epoch 21 loss: 0.0337\n",
      "epoch 22 loss: 0.0337\n",
      "epoch 23 loss: 0.0328\n",
      "epoch 24 loss: 0.0318\n",
      "epoch 25 loss: 0.0373\n",
      "epoch 26 loss: 0.0335\n",
      "epoch 27 loss: 0.0290\n",
      "epoch 28 loss: 0.0298\n",
      "epoch 29 loss: 0.0306\n",
      "epoch 30 loss: 0.0357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3h/5yfj7yqx7lg98t310bfxxr3m0000gn/T/ipykernel_54370/2623238862.py:92: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results_hybrid = pd.concat([results_hybrid, result]).reset_index(drop=True)\n",
      "06:30:25 - cmdstanpy - INFO - Chain [1] start processing\n",
      "Python(55290) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06:30:25 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 loss: 0.2157\n",
      "epoch 2 loss: 0.0810\n",
      "epoch 3 loss: 0.0613\n",
      "epoch 4 loss: 0.0473\n",
      "epoch 5 loss: 0.0605\n",
      "epoch 6 loss: 0.0651\n",
      "epoch 7 loss: 0.0387\n",
      "epoch 8 loss: 0.0430\n",
      "epoch 9 loss: 0.0461\n",
      "epoch 10 loss: 0.0465\n",
      "epoch 11 loss: 0.0403\n",
      "epoch 12 loss: 0.0479\n",
      "epoch 13 loss: 0.0399\n",
      "epoch 14 loss: 0.0370\n",
      "epoch 15 loss: 0.0313\n",
      "epoch 16 loss: 0.0489\n",
      "epoch 17 loss: 0.0468\n",
      "epoch 18 loss: 0.0365\n",
      "epoch 19 loss: 0.0338\n",
      "epoch 20 loss: 0.0329\n",
      "epoch 21 loss: 0.0356\n",
      "epoch 22 loss: 0.0354\n",
      "epoch 23 loss: 0.0349\n",
      "epoch 24 loss: 0.0341\n",
      "epoch 25 loss: 0.0324\n",
      "epoch 26 loss: 0.0380\n",
      "epoch 27 loss: 0.0347\n",
      "epoch 28 loss: 0.0344\n",
      "epoch 29 loss: 0.0376\n",
      "epoch 30 loss: 0.0302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06:34:40 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(55553) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "06:34:40 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 loss: 0.1677\n",
      "epoch 2 loss: 0.0778\n",
      "epoch 3 loss: 0.0509\n",
      "epoch 4 loss: 0.0485\n",
      "epoch 5 loss: 0.0436\n",
      "epoch 6 loss: 0.0449\n",
      "epoch 7 loss: 0.0402\n",
      "epoch 8 loss: 0.0379\n",
      "epoch 9 loss: 0.0370\n",
      "epoch 10 loss: 0.0433\n",
      "epoch 11 loss: 0.0405\n",
      "epoch 12 loss: 0.0510\n",
      "epoch 13 loss: 0.0361\n",
      "epoch 14 loss: 0.0346\n",
      "epoch 15 loss: 0.0373\n",
      "epoch 16 loss: 0.0377\n",
      "epoch 17 loss: 0.0397\n",
      "epoch 18 loss: 0.0360\n",
      "epoch 19 loss: 0.0338\n",
      "epoch 20 loss: 0.0381\n",
      "epoch 21 loss: 0.0352\n",
      "epoch 22 loss: 0.0357\n",
      "epoch 23 loss: 0.0350\n",
      "epoch 24 loss: 0.0349\n",
      "epoch 25 loss: 0.0345\n",
      "epoch 26 loss: 0.0329\n",
      "epoch 27 loss: 0.0316\n",
      "epoch 28 loss: 0.0316\n",
      "epoch 29 loss: 0.0306\n",
      "epoch 30 loss: 0.0356\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "59199403cec4893e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import Hierarchical_RNN_2stage_uncertainty as hrnn\n",
    "import os\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Configuration\n",
    "# ============================================================================\n",
    "\n",
    "fourier_config = hrnn.FourierConfig(\n",
    "    K_yearly_to_daily=8,\n",
    "    K_monthly_to_daily=6,\n",
    "    K_weekly_to_daily=8,\n",
    "    K_yearly_to_hourly=10,\n",
    "    K_monthly_to_hourly=7,\n",
    "    K_daily_to_hourly=8,\n",
    "    K_weekly_to_hourly=3,\n",
    ")\n",
    "\n",
    "train_config = hrnn.TrainingConfig(\n",
    "    base_epochs=200,\n",
    "    yearly_to_daily_multiplier=6,\n",
    "    daily_to_hourly_multiplier=1,\n",
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    lr=5e-4,\n",
    "    lambda0=1e-6,\n",
    "    lambdaf=1e-5,\n",
    "    batch_size=32,\n",
    "    T_seq_yearly=3,\n",
    "    T_seq_daily=32,\n",
    "    curriculum_start_prob=1.0,\n",
    "    curriculum_end_prob=1.0\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# Results Storage\n",
    "# ============================================================================\n",
    "\n",
    "results_hierarchical = pd.DataFrame({\n",
    "    'year': [],\n",
    "    'month': [],\n",
    "    'day': [],\n",
    "    'hour': [],\n",
    "    'y_hat': [],\n",
    "    'y': []\n",
    "})\n",
    "\n",
    "# ============================================================================\n",
    "# Rolling Window Setup\n",
    "# ============================================================================\n",
    "\n",
    "date_start = pd.to_datetime('2005-01-01 00:00:00')\n",
    "date_end = pd.to_datetime('2015-01-01 00:00:00')\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"2-Stage Hierarchical RNN - Deterministic Prediction\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Initial training window: {date_start.date()} to {date_end.date()}\")\n",
    "print(f\"Window size: ~10 years\")\n",
    "print(f\"Prediction: 1 year ahead (deterministic)\")\n",
    "print(f\"T_seq_yearly: {train_config.T_seq_yearly}\")\n",
    "print(f\"T_seq_daily: {train_config.T_seq_daily}\")\n",
    "print(f\"Device: {train_config.device}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(f'experiment_results/{out_dir}', exist_ok=True)\n",
    "\n",
    "# ============================================================================\n",
    "# Rolling Window Testing Loop\n",
    "# ============================================================================\n",
    "\n",
    "i = 1\n",
    "\n",
    "while True:\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"Iteration {i}\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Get training data\n",
    "    df_train = df_dayton.loc[(df_dayton['ds'] >= date_start) & (df_dayton['ds'] < date_end)].copy().sort_values(by='ds').reset_index(drop=True)\n",
    "\n",
    "    # Get test year\n",
    "    test_year_start = date_end\n",
    "    test_year_end = date_end + pd.DateOffset(years=1)\n",
    "    df_test = df_dayton.loc[(df_dayton['ds'] >= test_year_start) & (df_dayton['ds'] < test_year_end)].copy().sort_values(by='ds').reset_index(drop=True)\n",
    "\n",
    "    if len(df_test) == 0:\n",
    "        print(\"No more test data available. Stopping.\")\n",
    "        break\n",
    "\n",
    "    print(f\"Training: {date_start.date()} to {date_end.date()}\")\n",
    "    print(f\"Testing:  {test_year_start.date()} to {test_year_end.date()}\")\n",
    "    print(f\"Training samples: {len(df_train)}\")\n",
    "    print(f\"Test samples: {len(df_test)}\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # Build and Train Model\n",
    "    # ========================================================================\n",
    "\n",
    "    print(\"\\nBuilding model...\")\n",
    "    model_config = dict(\n",
    "        fourier_config=fourier_config,\n",
    "        training_config=train_config,\n",
    "        latent_dim=36,\n",
    "        d_model=128,\n",
    "        nhead=4\n",
    "    )\n",
    "    downscaler = hrnn.build_model_dp(hrnn.HierarchicalDownscaler_2Stage, **model_config)\n",
    "\n",
    "    # Train model using Approach 2 (Curriculum Learning)\n",
    "    print(\"\\nTraining model...\")\n",
    "    trainer = hrnn.Approach2_CurriculumLearning_2Stage(downscaler)\n",
    "    trainer.train(df_train)\n",
    "\n",
    "    # ========================================================================\n",
    "    # Generate Deterministic Prediction\n",
    "    # ========================================================================\n",
    "\n",
    "    test_year_int = test_year_start.year\n",
    "    yearly_sum = df_test['y'].sum()\n",
    "\n",
    "    print(f\"\\nTest year: {test_year_int}\")\n",
    "    print(f\"Actual yearly sum: {yearly_sum:.2f}\")\n",
    "\n",
    "    # Prepare historical data from training\n",
    "    data_dict_train = downscaler.module.prepare_data(df_train) if hasattr(downscaler, 'module') else downscaler.prepare_data(df_train)\n",
    "\n",
    "    # Get yearly history for Stage 1\n",
    "    df_yearly_train = data_dict_train['yearly'].sort_values('year')\n",
    "    historical_years = [{'year': row['year'], 'yearly_sum': row['yearly_sum']} for _, row in df_yearly_train.iterrows()]\n",
    "\n",
    "    print(f\"Using {len(historical_years)} historical years as context\")\n",
    "\n",
    "    print(f\"\\nGenerating deterministic prediction for year {test_year_int}...\")\n",
    "    predictor = hrnn.HierarchicalPredictor_2Stage(downscaler)\n",
    "\n",
    "    y_hat = predictor.predict_yearly_to_hourly(\n",
    "        yearly_sum=yearly_sum,\n",
    "        year=test_year_int,\n",
    "        historical_years=historical_years\n",
    "    )\n",
    "\n",
    "    print(f\"Generated predictions of shape {y_hat.shape}\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # Align with Test Data\n",
    "    # ========================================================================\n",
    "\n",
    "    n_hours_actual = len(df_test)\n",
    "    n_hours_pred = len(y_hat)\n",
    "\n",
    "    if n_hours_pred > n_hours_actual:\n",
    "        y_hat = y_hat[:n_hours_actual]\n",
    "        print(f\"Trimmed predictions from {n_hours_pred} to {n_hours_actual} hours\")\n",
    "    elif n_hours_pred < n_hours_actual:\n",
    "        # Pad if needed\n",
    "        padding_value = y_hat.mean()\n",
    "        pad_width = n_hours_actual - n_hours_pred\n",
    "        y_hat = np.pad(y_hat, (0, pad_width), 'constant', constant_values=padding_value)\n",
    "        print(f\"Padded predictions from {n_hours_pred} to {n_hours_actual} hours\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # Store Results\n",
    "    # ========================================================================\n",
    "\n",
    "    y_true = df_test['y'].values\n",
    "\n",
    "    result = pd.DataFrame({\n",
    "        'year': df_test['ds'].dt.year,\n",
    "        'month': df_test['ds'].dt.month,\n",
    "        'day': df_test['ds'].dt.day,\n",
    "        'hour': df_test['ds'].dt.hour,\n",
    "        'y_hat': y_hat,\n",
    "        'y': y_true\n",
    "    })\n",
    "\n",
    "    # Append to results\n",
    "    results_hierarchical = pd.concat(\n",
    "        [results_hierarchical, result],\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "    # ========================================================================\n",
    "    # Save Intermediate Results\n",
    "    # ========================================================================\n",
    "\n",
    "    if i % 1 == 0:\n",
    "        print(f\"\\nSaving intermediate results...\")\n",
    "        results_hierarchical.to_csv(\n",
    "            f'experiment_results/{out_dir}/results_hierarchical.csv',\n",
    "            index=False\n",
    "        )\n",
    "\n",
    "    # ========================================================================\n",
    "    # Move Window Forward\n",
    "    # ========================================================================\n",
    "\n",
    "    date_start += pd.DateOffset(years=1)\n",
    "    date_end += pd.DateOffset(years=1)\n",
    "\n",
    "    # Stop after 3 years of testing\n",
    "    if i == 3:\n",
    "        print('\\n' + '=' * 70)\n",
    "        print('Finished 3 iterations!')\n",
    "        print('=' * 70)\n",
    "        break\n",
    "    else:\n",
    "        i += 1\n",
    "\n",
    "# ============================================================================\n",
    "# Final Save\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Saving final results...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "results_hierarchical.to_csv(\n",
    "    f'experiment_results/{out_dir}/results_hierarchical.csv',\n",
    "    index=False\n",
    ")\n",
    "\n",
    "print(\"\\nAll results saved successfully!\")\n",
    "print(f\"Output directory: experiment_results/{out_dir}/\")\n",
    "print(\"\\nFiles created:\")\n",
    "print(\"  - results_hierarchical.csv\")"
   ],
   "id": "ba14caae12ee4b20"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Deok",
   "id": "45ce0547191c51d2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T13:11:57.311831Z",
     "start_time": "2025-11-11T13:11:56.218519Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from prophet import Prophet\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n"
   ],
   "id": "47d4c2b36a073502",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Importing plotly failed. Interactive plots will not work.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T13:11:58.853314Z",
     "start_time": "2025-11-11T13:11:58.849924Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def simple_dst_fix(df: pd.DataFrame, start_at_midnight: bool = True) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[\"ds\"] = pd.to_datetime(df[\"ds\"])\n",
    "    df = df.sort_values(\"ds\")\n",
    "\n",
    "    df = df[~df[\"ds\"].duplicated(keep=\"first\")]\n",
    "\n",
    "    start = df[\"ds\"].iloc[0]\n",
    "    if start_at_midnight:\n",
    "        start = start.normalize()\n",
    "    end = df[\"ds\"].iloc[-1]\n",
    "    full_idx = pd.date_range(start, end, freq=\"h\")\n",
    "\n",
    "    out = df.set_index(\"ds\").reindex(full_idx)\n",
    "\n",
    "    num_cols = out.select_dtypes(include=\"number\").columns\n",
    "    out[num_cols] = out[num_cols].ffill()\n",
    "\n",
    "    if out[num_cols].isna().any().any():\n",
    "        out[num_cols] = out[num_cols].bfill()\n",
    "\n",
    "    out = out.rename_axis(\"ds\").reset_index()\n",
    "\n",
    "    return out"
   ],
   "id": "3745789300592a4f",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T13:12:00.781419Z",
     "start_time": "2025-11-11T13:12:00.719292Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_deok = pd.read_csv(\"data/DEOK_hourly.csv\")\n",
    "df_deok.rename(columns={'Datetime': 'ds', 'DEOK_MW': 'y'}, inplace=True)\n",
    "df_deok['ds'] = pd.to_datetime(df_deok['ds'], format='%Y-%m-%d %H:%M:%S')\n",
    "df_deok = simple_dst_fix(df_deok)\n",
    "out_dir = 'Deok_results'"
   ],
   "id": "78603f2255f34384",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "df5f3471d0dc64c6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "6aa1294ad16b30ba"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T13:30:32.421056Z",
     "start_time": "2025-11-11T13:12:07.525928Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import prophet_linear_adjust_yearly_to_hourly\n",
    "\n",
    "yearly_demand = df_deok.groupby(df_deok['ds'].dt.year)['y'].sum().reset_index()\n",
    "date_start = pd.to_datetime('2012-01-01 00:00:00')\n",
    "date_end = pd.to_datetime('2015-01-01 00:00:00')\n",
    "\n",
    "results_prophet_yearly = []\n",
    "i = 1\n",
    "\n",
    "while True:\n",
    "    print(i)\n",
    "    result = prophet_linear_adjust_yearly_to_hourly.forecast_next_year_hourly(\n",
    "        df_deok, date_start, date_end, yearly_demand,\n",
    "        bayesian_samples=0,\n",
    "        manual=False,\n",
    "        daily=True,\n",
    "        weekly = True,\n",
    "        yearly = True,\n",
    "        monthly = True)\n",
    "\n",
    "    results_prophet_yearly.append(result)\n",
    "\n",
    "    date_start += pd.DateOffset(years=1)\n",
    "    date_end += pd.DateOffset(years=1)\n",
    "\n",
    "    if i == 3:\n",
    "        break\n",
    "    else:\n",
    "        i += 1\n",
    "\n",
    "results_prophet_yearly = pd.concat(results_prophet_yearly, ignore_index=True)\n",
    "results_prophet_yearly.to_csv('experiment_results/' + out_dir + '/results_prophet_yearly.csv', index=False)\n"
   ],
   "id": "3a58b9965940d1e4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07:12:08 - cmdstanpy - INFO - Chain [1] start processing\n",
      "07:12:14 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07:17:34 - cmdstanpy - INFO - Chain [1] start processing\n",
      "07:17:40 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07:23:51 - cmdstanpy - INFO - Chain [1] start processing\n",
      "07:23:57 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "a66095e7753ec07b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T13:35:02.375106Z",
     "start_time": "2025-11-11T13:30:59.786191Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import prophet_yearly_to_daily\n",
    "import RNN_fourier_RNN_uncertainty\n",
    "\n",
    "train_config = RNN_fourier_RNN_uncertainty.training_config(n_epochs=30, device=torch.device(\"mps\"))\n",
    "fourier_conf = RNN_fourier_RNN_uncertainty.fourier_config(mode=\"matrix\", K_weekly=3, K_monthly=6, K_yearly=10)\n",
    "\n",
    "K_total = fourier_conf.K_weekly + fourier_conf.K_monthly + fourier_conf.K_yearly\n",
    "F_per_hour = 2 * K_total\n",
    "\n",
    "if fourier_conf.mode == \"vector\":\n",
    "    cont_dim = 1 + F_per_hour\n",
    "    fourier_dim = F_per_hour\n",
    "else:\n",
    "    cont_dim = 1 + 24 * F_per_hour\n",
    "    fourier_dim = F_per_hour\n",
    "\n",
    "df_daily = df_deok.copy()\n",
    "df_daily['ds'] = pd.to_datetime(df_daily['ds'])\n",
    "df_daily['date'] = df_daily['ds'].dt.date\n",
    "df_daily_agg = df_daily.groupby('date')['y'].sum().reset_index()\n",
    "df_daily_agg.columns = ['ds', 'y']\n",
    "df_daily_agg['ds'] = pd.to_datetime(df_daily_agg['ds'])\n",
    "\n",
    "yearly_demand = df_daily_agg.copy()\n",
    "yearly_demand['year'] = yearly_demand['ds'].dt.year\n",
    "yearly_demand = yearly_demand.groupby('year')['y'].sum().reset_index()\n",
    "yearly_demand.columns = ['ds', 'y']\n",
    "\n",
    "results_hybrid = pd.DataFrame({'ds': [], 'y_hat': [], 'y': []})\n",
    "\n",
    "date_start = pd.to_datetime('2012-01-01 00:00:00')\n",
    "date_end = pd.to_datetime('2015-01-01 00:00:00')\n",
    "\n",
    "i = 1\n",
    "\n",
    "while True:\n",
    "\n",
    "    print(i)\n",
    "    df_train_day = df_daily_agg.loc[(df_daily_agg['ds'] >= date_start) & (df_daily_agg['ds'] < date_end)].copy()\n",
    "    df_test_day = df_daily_agg.loc[(df_daily_agg['ds'] >= date_end) & (df_daily_agg['ds'] < date_end + pd.DateOffset(years=1))].copy()\n",
    "\n",
    "    # ===== STAGE 1: Prophet predicts daily loads for the next year =====\n",
    "    result_prophet = prophet_yearly_to_daily.forecast_next_year_daily(\n",
    "        df_train_day,\n",
    "        df_test_day,\n",
    "        yearly_demand,\n",
    "        manual=False,\n",
    "        weekly=True,\n",
    "        monthly=True,\n",
    "        yearly=True\n",
    "    )\n",
    "    result_prophet = result_prophet.sort_values('ds').reset_index()\n",
    "\n",
    "    df_train_hour = df_deok.loc[(df_deok['ds'] >= date_start) & (df_deok['ds'] < date_end)].copy().sort_values('ds').reset_index(drop=True)\n",
    "    df_test_hour = df_deok.loc[(df_deok['ds'] >= date_end) & (df_deok['ds'] < date_end + pd.DateOffset(years=1))].copy().sort_values('ds').reset_index(drop=True)\n",
    "\n",
    "    model = RNN_fourier_RNN_uncertainty.RNN_fourier(\n",
    "        cont_dim=cont_dim,\n",
    "        fourier_dim=fourier_dim,\n",
    "        xf_mode=\"matrix\",\n",
    "        d_model = 128,\n",
    "        latent_dim=32,\n",
    "        nhead=4\n",
    "    )\n",
    "\n",
    "    trainer = RNN_fourier_RNN_uncertainty.RNN_train_fourier(model, train_config, fourier_conf)\n",
    "    trainer(df_train_hour)\n",
    "\n",
    "    daily_prediction = result_prophet[['ds','adjusted_y']]\n",
    "    daily_prediction.columns = ['ds', 'y']\n",
    "\n",
    "    fake_hourly_load = []\n",
    "    for _, row in daily_prediction.iterrows():\n",
    "        day = row['ds']\n",
    "        daily_val = row['y'] / 24\n",
    "        for h in range(24):\n",
    "            fake_hourly_load.append({\n",
    "                'ds': pd.Timestamp(day) + pd.Timedelta(hours=h),\n",
    "                'y': daily_val\n",
    "            })\n",
    "\n",
    "    # convert to DataFrame\n",
    "    fake_hourly_load = pd.DataFrame(fake_hourly_load)\n",
    "\n",
    "    forcast, _ = trainer.forecate(fake_hourly_load, deterministic = True)\n",
    "\n",
    "    result = pd.DataFrame({'ds': df_test_hour['ds'], 'y_hat': forcast, 'y': df_test_hour['y']})\n",
    "\n",
    "    results_hybrid = pd.concat([results_hybrid, result]).reset_index(drop=True)\n",
    "\n",
    "    date_start += pd.DateOffset(years=1)\n",
    "    date_end += pd.DateOffset(years=1)\n",
    "\n",
    "    if i  == 3:\n",
    "        break\n",
    "\n",
    "    else:\n",
    "        i = i + 1\n",
    "\n",
    "results_hybrid.to_csv('experiment_results/' + out_dir + '/results_prophet_rnn_hybrid.csv', index=False)"
   ],
   "id": "e74140635631ccb9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07:30:59 - cmdstanpy - INFO - Chain [1] start processing\n",
      "Python(57461) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "07:31:00 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zero/Documents/GitHub/RNN_based_downscaling/RNN_fourier_RNN_uncertainty.py:495: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.\n",
      "Consider using tensor.detach() first. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/python_variable_methods.cpp:837.)\n",
      "  print(f\"epoch {ep + 1} loss: {float(loss):.4f}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 loss: 0.4490\n",
      "epoch 2 loss: 0.3370\n",
      "epoch 3 loss: 0.2543\n",
      "epoch 4 loss: 0.2151\n",
      "epoch 5 loss: 0.2006\n",
      "epoch 6 loss: 0.2084\n",
      "epoch 7 loss: 0.1880\n",
      "epoch 8 loss: 0.1303\n",
      "epoch 9 loss: 0.1319\n",
      "epoch 10 loss: 0.1414\n",
      "epoch 11 loss: 0.1184\n",
      "epoch 12 loss: 0.0698\n",
      "epoch 13 loss: 0.0660\n",
      "epoch 14 loss: 0.0632\n",
      "epoch 15 loss: 0.0692\n",
      "epoch 16 loss: 0.0644\n",
      "epoch 17 loss: 0.0765\n",
      "epoch 18 loss: 0.0566\n",
      "epoch 19 loss: 0.0606\n",
      "epoch 20 loss: 0.0558\n",
      "epoch 21 loss: 0.0605\n",
      "epoch 22 loss: 0.0605\n",
      "epoch 23 loss: 0.0541\n",
      "epoch 24 loss: 0.0569\n",
      "epoch 25 loss: 0.0582\n",
      "epoch 26 loss: 0.0500\n",
      "epoch 27 loss: 0.0534\n",
      "epoch 28 loss: 0.0605\n",
      "epoch 29 loss: 0.0584\n",
      "epoch 30 loss: 0.0454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3h/5yfj7yqx7lg98t310bfxxr3m0000gn/T/ipykernel_56714/2669931423.py:92: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results_hybrid = pd.concat([results_hybrid, result]).reset_index(drop=True)\n",
      "07:32:17 - cmdstanpy - INFO - Chain [1] start processing\n",
      "Python(57518) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07:32:17 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 loss: 0.6786\n",
      "epoch 2 loss: 0.3461\n",
      "epoch 3 loss: 0.2497\n",
      "epoch 4 loss: 0.2290\n",
      "epoch 5 loss: 0.1850\n",
      "epoch 6 loss: 0.1247\n",
      "epoch 7 loss: 0.1934\n",
      "epoch 8 loss: 0.1004\n",
      "epoch 9 loss: 0.0777\n",
      "epoch 10 loss: 0.0702\n",
      "epoch 11 loss: 0.0746\n",
      "epoch 12 loss: 0.0632\n",
      "epoch 13 loss: 0.0650\n",
      "epoch 14 loss: 0.0703\n",
      "epoch 15 loss: 0.0635\n",
      "epoch 16 loss: 0.0510\n",
      "epoch 17 loss: 0.0534\n",
      "epoch 18 loss: 0.0537\n",
      "epoch 19 loss: 0.0492\n",
      "epoch 20 loss: 0.0575\n",
      "epoch 21 loss: 0.0541\n",
      "epoch 22 loss: 0.0466\n",
      "epoch 23 loss: 0.0516\n",
      "epoch 24 loss: 0.0454\n",
      "epoch 25 loss: 0.0515\n",
      "epoch 26 loss: 0.0583\n",
      "epoch 27 loss: 0.0497\n",
      "epoch 28 loss: 0.0474\n",
      "epoch 29 loss: 0.0462\n",
      "epoch 30 loss: 0.0453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07:33:40 - cmdstanpy - INFO - Chain [1] start processing\n",
      "Python(57599) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "07:33:40 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "epoch 1 loss: 0.5050\n",
      "epoch 2 loss: 0.2192\n",
      "epoch 3 loss: 0.1911\n",
      "epoch 4 loss: 0.1137\n",
      "epoch 5 loss: 0.0955\n",
      "epoch 6 loss: 0.0971\n",
      "epoch 7 loss: 0.0749\n",
      "epoch 8 loss: 0.0720\n",
      "epoch 9 loss: 0.0630\n",
      "epoch 10 loss: 0.0712\n",
      "epoch 11 loss: 0.0627\n",
      "epoch 12 loss: 0.0545\n",
      "epoch 13 loss: 0.0619\n",
      "epoch 14 loss: 0.0865\n",
      "epoch 15 loss: 0.0469\n",
      "epoch 16 loss: 0.0427\n",
      "epoch 17 loss: 0.0537\n",
      "epoch 18 loss: 0.0670\n",
      "epoch 19 loss: 0.0601\n",
      "epoch 20 loss: 0.0503\n",
      "epoch 21 loss: 0.0678\n",
      "epoch 22 loss: 0.0424\n",
      "epoch 23 loss: 0.0453\n",
      "epoch 24 loss: 0.0416\n",
      "epoch 25 loss: 0.0475\n",
      "epoch 26 loss: 0.0408\n",
      "epoch 27 loss: 0.0433\n",
      "epoch 28 loss: 0.0383\n",
      "epoch 29 loss: 0.0375\n",
      "epoch 30 loss: 0.0313\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "eb4e8b9903a266a9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import Hierarchical_RNN_2stage_uncertainty as hrnn\n",
    "import os\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Configuration\n",
    "# ============================================================================\n",
    "\n",
    "fourier_config = hrnn.FourierConfig(\n",
    "    K_yearly_to_daily=8,\n",
    "    K_monthly_to_daily=6,\n",
    "    K_weekly_to_daily=8,\n",
    "    K_yearly_to_hourly=10,\n",
    "    K_monthly_to_hourly=7,\n",
    "    K_daily_to_hourly=8,\n",
    "    K_weekly_to_hourly=3,\n",
    ")\n",
    "\n",
    "train_config = hrnn.TrainingConfig(\n",
    "    base_epochs=200,\n",
    "    yearly_to_daily_multiplier=6,\n",
    "    daily_to_hourly_multiplier=1,\n",
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    lr=5e-4,\n",
    "    lambda0=1e-6,\n",
    "    lambdaf=1e-5,\n",
    "    batch_size=32,\n",
    "    T_seq_yearly=3,\n",
    "    T_seq_daily=32,\n",
    "    curriculum_start_prob=1.0,\n",
    "    curriculum_end_prob=1.0\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# Results Storage\n",
    "# ============================================================================\n",
    "\n",
    "results_hierarchical = pd.DataFrame({\n",
    "    'year': [],\n",
    "    'month': [],\n",
    "    'day': [],\n",
    "    'hour': [],\n",
    "    'y_hat': [],\n",
    "    'y': []\n",
    "})\n",
    "\n",
    "# ============================================================================\n",
    "# Rolling Window Setup\n",
    "# ============================================================================\n",
    "\n",
    "date_start = pd.to_datetime('2012-01-01 00:00:00')\n",
    "date_end = pd.to_datetime('2015-01-01 00:00:00')\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"2-Stage Hierarchical RNN - Deterministic Prediction\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Initial training window: {date_start.date()} to {date_end.date()}\")\n",
    "print(f\"Window size: ~10 years\")\n",
    "print(f\"Prediction: 1 year ahead (deterministic)\")\n",
    "print(f\"T_seq_yearly: {train_config.T_seq_yearly}\")\n",
    "print(f\"T_seq_daily: {train_config.T_seq_daily}\")\n",
    "print(f\"Device: {train_config.device}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(f'experiment_results/{out_dir}', exist_ok=True)\n",
    "\n",
    "# ============================================================================\n",
    "# Rolling Window Testing Loop\n",
    "# ============================================================================\n",
    "\n",
    "i = 1\n",
    "\n",
    "while True:\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"Iteration {i}\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Get training data\n",
    "    df_train = df_deok.loc[(df_deok['ds'] >= date_start) & (df_deok['ds'] < date_end)].copy().sort_values(by='ds').reset_index(drop=True)\n",
    "\n",
    "    # Get test year\n",
    "    test_year_start = date_end\n",
    "    test_year_end = date_end + pd.DateOffset(years=1)\n",
    "    df_test = df_deok.loc[(df_deok['ds'] >= test_year_start) & (df_deok['ds'] < test_year_end)].copy().sort_values(by='ds').reset_index(drop=True)\n",
    "\n",
    "    if len(df_test) == 0:\n",
    "        print(\"No more test data available. Stopping.\")\n",
    "        break\n",
    "\n",
    "    print(f\"Training: {date_start.date()} to {date_end.date()}\")\n",
    "    print(f\"Testing:  {test_year_start.date()} to {test_year_end.date()}\")\n",
    "    print(f\"Training samples: {len(df_train)}\")\n",
    "    print(f\"Test samples: {len(df_test)}\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # Build and Train Model\n",
    "    # ========================================================================\n",
    "\n",
    "    print(\"\\nBuilding model...\")\n",
    "    model_config = dict(\n",
    "        fourier_config=fourier_config,\n",
    "        training_config=train_config,\n",
    "        latent_dim=36,\n",
    "        d_model=128,\n",
    "        nhead=4\n",
    "    )\n",
    "    downscaler = hrnn.build_model_dp(hrnn.HierarchicalDownscaler_2Stage, **model_config)\n",
    "\n",
    "    # Train model using Approach 2 (Curriculum Learning)\n",
    "    print(\"\\nTraining model...\")\n",
    "    trainer = hrnn.Approach2_CurriculumLearning_2Stage(downscaler)\n",
    "    trainer.train(df_train)\n",
    "\n",
    "    # ========================================================================\n",
    "    # Generate Deterministic Prediction\n",
    "    # ========================================================================\n",
    "\n",
    "    test_year_int = test_year_start.year\n",
    "    yearly_sum = df_test['y'].sum()\n",
    "\n",
    "    print(f\"\\nTest year: {test_year_int}\")\n",
    "    print(f\"Actual yearly sum: {yearly_sum:.2f}\")\n",
    "\n",
    "    # Prepare historical data from training\n",
    "    data_dict_train = downscaler.module.prepare_data(df_train) if hasattr(downscaler, 'module') else downscaler.prepare_data(df_train)\n",
    "\n",
    "    # Get yearly history for Stage 1\n",
    "    df_yearly_train = data_dict_train['yearly'].sort_values('year')\n",
    "    historical_years = [{'year': row['year'], 'yearly_sum': row['yearly_sum']} for _, row in df_yearly_train.iterrows()]\n",
    "\n",
    "    print(f\"Using {len(historical_years)} historical years as context\")\n",
    "\n",
    "    print(f\"\\nGenerating deterministic prediction for year {test_year_int}...\")\n",
    "    predictor = hrnn.HierarchicalPredictor_2Stage(downscaler)\n",
    "\n",
    "    y_hat = predictor.predict_yearly_to_hourly(\n",
    "        yearly_sum=yearly_sum,\n",
    "        year=test_year_int,\n",
    "        historical_years=historical_years\n",
    "    )\n",
    "\n",
    "    print(f\"Generated predictions of shape {y_hat.shape}\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # Align with Test Data\n",
    "    # ========================================================================\n",
    "\n",
    "    n_hours_actual = len(df_test)\n",
    "    n_hours_pred = len(y_hat)\n",
    "\n",
    "    if n_hours_pred > n_hours_actual:\n",
    "        y_hat = y_hat[:n_hours_actual]\n",
    "        print(f\"Trimmed predictions from {n_hours_pred} to {n_hours_actual} hours\")\n",
    "    elif n_hours_pred < n_hours_actual:\n",
    "        # Pad if needed\n",
    "        padding_value = y_hat.mean()\n",
    "        pad_width = n_hours_actual - n_hours_pred\n",
    "        y_hat = np.pad(y_hat, (0, pad_width), 'constant', constant_values=padding_value)\n",
    "        print(f\"Padded predictions from {n_hours_pred} to {n_hours_actual} hours\")\n",
    "\n",
    "    # ========================================================================\n",
    "    # Store Results\n",
    "    # ========================================================================\n",
    "\n",
    "    y_true = df_test['y'].values\n",
    "\n",
    "    result = pd.DataFrame({\n",
    "        'year': df_test['ds'].dt.year,\n",
    "        'month': df_test['ds'].dt.month,\n",
    "        'day': df_test['ds'].dt.day,\n",
    "        'hour': df_test['ds'].dt.hour,\n",
    "        'y_hat': y_hat,\n",
    "        'y': y_true\n",
    "    })\n",
    "\n",
    "    # Append to results\n",
    "    results_hierarchical = pd.concat(\n",
    "        [results_hierarchical, result],\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "    # ========================================================================\n",
    "    # Save Intermediate Results\n",
    "    # ========================================================================\n",
    "\n",
    "    if i % 1 == 0:\n",
    "        print(f\"\\nSaving intermediate results...\")\n",
    "        results_hierarchical.to_csv(\n",
    "            f'experiment_results/{out_dir}/results_hierarchical.csv',\n",
    "            index=False\n",
    "        )\n",
    "\n",
    "    # ========================================================================\n",
    "    # Move Window Forward\n",
    "    # ========================================================================\n",
    "\n",
    "    date_start += pd.DateOffset(years=1)\n",
    "    date_end += pd.DateOffset(years=1)\n",
    "\n",
    "    # Stop after 3 years of testing\n",
    "    if i == 3:\n",
    "        print('\\n' + '=' * 70)\n",
    "        print('Finished 3 iterations!')\n",
    "        print('=' * 70)\n",
    "        break\n",
    "    else:\n",
    "        i += 1\n",
    "\n",
    "# ============================================================================\n",
    "# Final Save\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Saving final results...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "results_hierarchical.to_csv(\n",
    "    f'experiment_results/{out_dir}/results_hierarchical.csv',\n",
    "    index=False\n",
    ")\n",
    "\n",
    "print(\"\\nAll results saved successfully!\")\n",
    "print(f\"Output directory: experiment_results/{out_dir}/\")\n",
    "print(\"\\nFiles created:\")\n",
    "print(\"  - results_hierarchical.csv\")"
   ],
   "id": "c3dba87ef87edc07"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
